<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Yacana - Agents & Tasks</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="../assets/css/main.css"/>
    <link rel="stylesheet" href="../assets/css/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/monokai.min.css">
    <link rel="stylesheet" href="../assets/css/foldgutter.min.css">
    <link rel="stylesheet" href="../assets/css/codemirror-custom.css">
    <link rel="stylesheet" href="../assets/css/zenburn.min.css">
    <link rel="shortcut icon" href="../images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="../images/favicon.ico" type="image/x-icon">
    <script src="../assets/js/codemirror.min.js"></script>
    <script src="../assets/js/python.min.js"></script>
    <script src="../assets/js/foldcode.min.js"></script>
    <script src="../assets/js/foldgutter.min.js"></script>
    <script src="../assets/js/brace-fold.min.js"></script>
    <script src="../assets/js/codemirror-custom.js"></script>
</head>

<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">
        <div class="inner">

            <!-- Header -->
            <header id="header">
                <a href="../index.html" class="logo"><strong>Yacana</strong>, powering open source LLMs</a>
                <ul class="icons">
                    <li><a href="https://x.com/RSoftwares_ofc" class="icon brands fa-twitter"><span class="label">Twitter</span></a>
                    </li>
                    <li><a href="https://medium.com/@docteur_rs" class="icon brands fa-medium-m"><span class="label">Medium</span></a>
                    </li>
                    <li><a href="https://www.youtube.com/channel/UCvi7R0CRmtxhWOVw62XteTw"
                           class="icon brands fa-youtube"><span class="label">Medium</span></a></li>
                    <li><a href="https://github.com/rememberSoftwares/yacana" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                </ul>
            </header>

            <!-- Content -->
            <section>
                <header class="main">
                    <h1 id="creating-an-agent">II. Agents & Tasks</h1>
                </header>

                <span class="image main"><img src="../images/agents_and_tasks.jpg"
                                              alt="Creating Agents and Tasks to solve"/></span>


                <h2>Creating an Agent</h2>

                <p>Now that you have an Ollama server running and Yacana installed let's create our first agent!</p>
                <p>Create a Python file with this content:</p>
                <pre><code class="language-python">
from yacana import OllamaAgent

agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:11434")
					</code></pre>
                <br>
                <p>The <code>OllamaAgent()</code> class takes...</p>
                <span class="icon solid fa-chevron-right"> 2 <u>mandatory</u> parameters:</span>
                <ol>
                    <li><strong>The agent name:</strong> Choose something short about the agent's global focus</li>
                    <li><strong>A model name:</strong> The Ollama model that this Agent will use. You may have
                        multiple Agents running different models. Some models are better suited for some specific
                        jobs so it can be interesting to mix LLM models. Use <code>ollama list</code> to list the
                        models you have downloaded.
                    </li>
                </ol>
                <span class="icon solid fa-chevron-right"> many <u>optional</u> parameters that we will discover in
						this tutorial. Here we
						can see 2 of them:</span>
                <ol>
                    <li><strong>The system prompt:</strong> Helps define the personality of the Agent.</li>
                    <li><strong>The endpoint:</strong> The URL of your Ollama instance. It points by default to your
                        localhost and on the Ollama default port. If you are using Ollama on your computer you can
                        remove this optional parameter and the default value will be used.
                    </li>
                </ol>

                <hr class="major"/>

                <h2 id="creating-tasks">Creating Tasks</h2>
                <p>The whole concept of the framework lies here. If you understand this following section then you
                    have mastered 80% of Yacana's building principle. Like in LangGraph, where you create nodes that
                    you link together, Yacana has a Task() class which takes as argument a task to solve. There are
                    no hardcoded links between the Tasks so it's easy to refactor and move things around. The
                    important concept to grasp here is that through these Tasks you will give instructions to the
                    LLM in a way that the result must be computable. Meaning instructions must be clear and the
                    prompt to use must reflect that. It's a Task, it's a job, it's something that needs solving but
                    written like it is given as an order! Let's see some examples :</p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

# First, let's make a basic AI agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Now we create a task and assign the agent1 to the task
task1 = Task(f"Solve the equation 2 + 2 and output the result", agent1)

# For something to happen, you must call the .solve() method on your task.
task1.solve()
					</code></pre>
                <br>
                <p>What's happening above?</p>
                <ul>
                    <li>First, we instantiated an Agent with the <code>llama3.1:8b</code> model. You might need to
                        update that depending on what LLM you downloaded from Ollama ;
                    </li>
                    <li>Second, we instantiated a Task ;</li>
                    <li>Third, we asked that the Task be solved ;</li>
                </ul>
                <p class="icon solid fa-info-circle"> For easing the learning curve the default logging level is
                    INFO. It will show what is going on in Yacana. Note that NOT ALL prompts are shown.</p>
                <p>The output should look like this:</p>
                <pre><code class="text-output">
INFO: [PROMPT][To: AI assistant]: Solve the equation 2 + 2 and output the result

INFO: [AI_RESPONSE][From: AI assistant]: The answer to the equation 2 + 2 is... (drumroll please)... 4!
							
So, the result of solving the equation 2 + 2 is indeed 4.
							</code></pre>
                <br>
                <p>If your terminal is working normally <span class="text-green">you should see the task's prompts in green and starting with
                    the '[PROMPT]' string.</span> <span class="text-purple">The LLM's answer should appear purple and start with the [AI_RESPONSE]
                    string.</span></p>
                <h4 id="task-parameters">Task parameters</h4>
                <p>The Task class takes 2 mandatory parameters:</p>
                <ul>
                    <li>The prompt: It is the task to be solved. Use imperative language, be precise, and ask for
                        step-by-step thinking for complex Tasks and expected outputs if needed.
                    </li>
                    <li>The agent that will be assigned to this task. The agent will be in charge of
                        solving the task.
                    </li>
                </ul>
                <p class="icon solid fa-info-circle"> Many other parameters can be given to a Task. We will see some
                    of them in the following sections of this tutorial. But you can already check out the 
                    <a href="classes.html#tech-doc-task">Task class</a>
                    documentation.</p>
                <h4 id="in-what-way-is-this-disruptive-compared-to-other-frameworks">In what way is this disruptive
                    compared to other frameworks?</h4>
                <p>In the above code snippet, we assigned the agent to the Task. So it's the Task that leads the
                    direction that the AI takes. In most other frameworks it's the opposite. You assign work to an
                    existing agent. This reversed way allows to have fine-grained control on each resolution step
                    as the LLM only follows breadcrumbs (the tasks). The pattern will become even more obvious as we
                    get to the Tool section of this tutorial. As you'll see the Tools are also assigned at the Task
                    level and not to the Agent directly.</p>
                <p>Compared with LangGraph, we cannot generate a call graph as an image because we don't
                    bind the tasks together explicitly. However, Yacana's way gives more flexibility and allows a
                    hierarchical programming way of scheduling the tasks and keeping control of the flow. It also
                    allows creating new Tasks dynamically if the need arises. You shall rely on your programming
                    skills and good OOP to have a clean code and good Task ordering. Yacana will never propose
                    any hardlink code or flat configurations.</p>

                <hr class="major"/>

                <h2 id="getting-the-result-of-a-task">Getting the result of a Task</h2>
                <p>Although the logs appear in the terminal's standard output, we still need to extract the LLM's response to the Task in order to use it.<br>
                    Getting the string message out of it is quite easy as the <code>.solve()</code> method returns a <code>Message()</code> class.<br>
                    Maybe you are thinking &quot;Ho nooo, another class to deal with&quot;. Well, let me tell you
                    that it's always better to have an OOP class than some semi-random Python dictionary where
                    you'll forget what keys it contains in a matter of minutes. Also, the Message class is very
                    straightforward. It exposes a <code>content</code> attribute. Update the current code to look
                    like this:</p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task, Message

# First, let's make a basic AI agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Now we create a task and assign the agent1 to the task
task1 = Task(f"Solve the equation 2 + 2 and output the result", agent1)

# So that something actually happens you must call the .solve() method on your task
my_message: Message = task1.solve()

# Printing the LLM's response
print(f"The AI response to our task is : {my_message.content}")
						</code></pre>
                <br>
                <p>There you go! Give it a try.</p>
                <p class="icon solid fa-info-circle"> Note that we used duck typing to postfix all variables
                    declaration with their type <code>my_message: Message</code>. Yacana's source code is entirely
                    duck-typed so that your IDE always knows what type it's dealing with and proposes the best
                    methods and arguments. We recommend that you do the same as it's the industry's best standards.
                </p>
                <hr>
                <p>Don't like having 100 lines of code for something simple? Then chain them all!</p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

# First, let's make a basic AI agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Creating the task, solving it, extracting the result
result: str = Task(f'Solve the equation 2 + 2 and output the result', agent1).solve().content
# Print the result
print(f"The AI response to our task is: {result}")
					</code></pre>

                <hr class="major"/>

                <h2 id="chaining-tasks">Chaining Tasks</h2>
                <p>
                    Agents keep track of the History of what they did (aka, all the Tasks they solved). So just call a second
                    Task and assign the same Agent. For instance, let's multiply by 2 the result of the initial
                    Task. Append this to our current script:</p>
                <pre><code class="language-python">
task2_result: str = Task(f'Multiply by 2 our previous result', agent1).solve().content
print(f"The AI response to our second task is : {task2_result}")
					</code></pre>
                <br>
                <p>You should get:</p>
                <pre><code class="text-output">
The AI response to our task is: If we multiply the previous result of 4 by 2, we get:
							
8
					</code></pre>
                <br>
                <p class="icon solid fa-info-circle"> Without tools this only relies on the LLM's ability to do the
                    maths and is dependent on its training.</p>
                <br>
                <p>See? The assigned Agent remembered that it had solved Task1 previously and used this
                    information to solve the second task.<br>
                    You can chain as many Tasks as you need. You can build anything now!</p>

                <hr class="major"/>

                <h2 id="logging-levels">Logging levels</h2>
                <p>As entering the AI landscape can get a bit hairy we decided to leave the INFO log level by
                    default. This allows to log to the standard output all the requests made to the LLM.<br>
                    Note that NOT everything of Yacana's internal magic appears in these logs. We don't show
                    everything because many time-traveling things are going around inside the history of an Agent and
                    printing a log at the time it is generated wouldn't always make sense.<br>
                    However, we try to log a maximum of information to help you understand what is happening
                    internally and allow you to tweak your prompts accordingly.</p>
                <p>Nonetheless, you are the master of what is logged and what isn't. You cannot let Yacana log
                    anything while working with an app in production.<br>
                    There are 5 levels of logs:</p>
                <ol>
                    <li><code>"DEBUG"</code></li>
                    <li><code>"INFO"</code> <span class="icon solid fa-chevron-left"></span> Default</li>
                    <li><code>"WARNING"</code></li>
                    <li><code>"ERROR"</code></li>
                    <li><code>"CRITICAL"</code></li>
                    <li><code>None</code> <span class="icon solid fa-chevron-left"></span> No logs</li>
                </ol>
                <p>To configure the log simply add this line at the start of your program:</p>
                <pre><code class="language-python">
from yacana import LoggerManager

LoggerManager.set_log_level("INFO")
					</code></pre>
                <br>
                <p class="icon solid fa-info-circle">Note that Yacana utilizes the Python logging package. This
                    means that setting the level to &quot;DEBUG&quot; will print other libraries' logs on the debug
                    level too.</p>
                <p>If you need a library to stop spamming, you can try the following:</p>
                <pre><code class="language-python">
from yacana import LoggerManager

LoggerManager.set_library_log_level("httpx", "WARNING")
					</code></pre>
                <br>
                <p>The above example sets the logging level of the network httpx library to warning, thus reducing
                    the log spamming.</p>
                
                <hr>

                <h3>Let's build</h3>
                <p>
                    Using what we know, let's build a simple chat interface:
                    <pre><code class="language-python">
from yacana import OllamaAgent, Task, GenericMessage, LoggerManager

LoggerManager.set_log_level(None)

ollama_agent = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:11434")
print("Ask me questions!")
while True:
    user_input = input("> ")
    message: GenericMessage = Task(user_input, ollama_agent).solve()
    print(message.content)
                    </code></pre>
                    Output:
                    <pre><code class="text-output">
Ask me questions!
> Why do boats float (short answer)
Boats float due to their displacement in water, which creates an upward buoyant force equal to the weight of the fluid (water) displaced. According to Archimedes' Principle, any object partially or fully submerged in a fluid will experience an upward buoyant force that equals the weight of the fluid it displaces.
>
                    </code></pre>
                    If we change the agent's system prompt we can talk to a pirate!
                    <pre><code class="language-python">
ollama_agent = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a pirate", endpoint="http://127.0.0.1:11434")
                    </code></pre><br>
                    Output:
                    <pre><code class="text-output">
Ask me questions!
> Where is the map ?
*looks around cautiously, then leans in close*

Ahoy, matey! I be thinkin' ye be lookin' fer me trusty treasure map, eh? *winks*

Alright, I'll let ye in on a little secret. It's hidden... *pauses for dramatic effect* ...on the island o' Tortuga! Ye can find it at the old windmill on the outskirts o' town. But be warned, matey: ye won't be the only scurvy dog afterin' that map!

Now, I be trustin' ye to keep this little chat between us, savvy?
>
                    </code></pre>

                <hr class="major"/>

                <h2 id="configuring-llms-settings">Configuring LLM's settings</h2>
                <p>
                    For advanced users, Yacana provides a way to tweak the LLM runtime behavior!<br>
                    For instance, lowering the <code>temperature</code> setting makes the model less creative in its
                    responses. On the contrary, raising this setting will make the LLM more chatty and creative.<br>
                    Yacana provides you with a class that exposes all the possible LLM properties. Also, if you need a
                    good explanation for each of them I would
                    recommend the <a href="https://youtu.be/QfFRNF5AhME?si=lpSYUq2WoidYiqzP">excellent video</a>
                    Matt Williams did on this subject.</p>
                <p class="icon solid fa-info-circle"> These settings are set at the Agent level so that you can have
                    the same underlying model used by two separate agents and have them behave differently.</p>
                <p class="icon solid fa-info-circle"> The <a href="classes.html#tech-doc-ollamamodelsettings">
                    OllamaModelSettings</a> class is tailored for the Ollama backend.
                    You can use <a href="classes.html#tech-doc-openaimodelsettings">OpenAiModelSettings</a> to
                    configure non-Ollama LLMs with their own set of available settings.</p>
                <p>We use the <a href="classes.html#tech-doc-ollamamodelsettings">OllamaModelSettings</a> class to configure the settings we need.</p>
                <p>For example, let's lower the temperature of an Agent to 0.4:</p>
                <pre><code class="language-python">
from yacana import OllamaModelSettings, OllamaAgent

ms = OllamaModelSettings(temperature=0.4)

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b", model_settings=ms)
					</code></pre>
                <br>
                <p>If you're wondering what the default values of these are when not set. Well, Ollama sets the
                    defaults for you. They can also be overridden in the Model config file (looks like a dockerfile
                    but for LLMs) and finally, you can set them through Yacana during runtime.</p>
                <p>A good way to show how this can have a real impact on the output is by setting the
                    <code>num_predict</code> parameter. This one allows control of how many tokens should be
                    generated by the LLM. Let's make the same Task twice but with different <code>num_predict</code>
                    values:
                </p>
                <pre><code class="language-python">
from yacana import OllamaModelSettings, OllamaAgent, Task

# Setting temperature and max token to 100
ms = OllamaModelSettings(temperature=0.4, num_predict=100)

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b", model_settings=ms)
Task("Why is the sky blue ?", agent1).solve()

print("-------------------")

# Settings max token to 15
ms = OllamaModelSettings(num_predict=15)

agent2 = OllamaAgent("Ai assistant", "llama3.1:8b", model_settings=ms)
Task("Why is the sky blue ?", agent2).solve()
					</code></pre>
                <br>
                <span>‚ñ∂Ô∏è Output:</span>
                <pre><code class="text-output">
INFO: [PROMPT]: Why is the sky blue ?

INFO: [AI_RESPONSE]: The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. Here's what happens:

1. **Sunlight**: When sunlight enters Earth's atmosphere, it contains all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).
2. **Molecules**: The atmosphere is made up of tiny molecules of gases like nitrogen (N2) and oxygen (O2). These molecules are much smaller than

-------------------

INFO: [PROMPT]: Why is the sky blue ?

INFO: [AI_RESPONSE]: The sky appears blue because of a phenomenon called Rayleigh scattering, named after
					</code></pre>
                <br>
                <p>
                    As you can see above the two agents didn't output the same number of tokens.
                    <br>
                    For OpenAiAgent the method is the same. But instead of using <code>OllamaModelSettings</code> you can use <code>OpenAiModelSettings</code>. For more information please refer to the <a href="other_inference_servers.html">Other inference servers</a> section.
                </p>
                <hr class="major"/>


                <h2 id="accessing-the-underlying-client-library">Accessing the underlying client library</h2>
                <p>
                    Yacana cannot recreate every functionality made available by the underlying client library. This is why we provide you with
                    direct access to the underlying library. This way, if you need more control over the LLM, you can set the desired parameters yourself.
                    <br>
                    <ul>
                        <li>For the <a href="classes.html#tech-doc-OllamaAgent">OllamaAgent</a>, the library used is <a href="https://github.com/ollama/ollama-python">python ollama</a>.</li>
                        <li>For the <a href="classes.html#tech-doc-openai-agent">OpenAIAgent</a>, the library used is <a href="https://github.com/openai/openai-python">python openai</a>.</li>
                    </ul>
                    You can pass any parameter supported by the underlying library to the Agent during runtime.
                    <br>
                    <br>
                    Configuration of the runtime can be done at two separate levels using the <code>runtime_config</code> parameter:
                    <ul>
                        <li>At the <b>Agent level</b>:
                    </ul>
                    <pre><code class="language-python">
vllm_agent = OpenAiAgent("AI assistant", "meta-llama/Llama-3.1-8B-Instruct", endpoint="http://127.0.0.1:8000/v1", api_token="", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})
                    </code></pre>
                    <br>
                    <ul>
                        <li>At the <b>task level</b>:
                    </ul>
                    <pre><code class="language-python">
Task("Tell me 2 facts about Canada.", agent, runtime_config={"extra_body": {'guided_decoding_backend': 'xgrammar'}}).solve()
                    </code></pre>
                    <br>
                    Configuration at the Agent level will be effective during the whole agent's life span.
                    <br>
                    Configuration at the task level will be effective for the current task only and will override parameters given at Agent level.
                    <br>
                    To look at a more complete example, please refer to the <a href="other_inference_servers.html#using-vllm">VLLM inference server</a> section.
                </p>
                <br>

                <header class="main">
                    <h2 id="concepts-of-routing">Routing</h2>
                </header>

                <span class="image main"><img src="../images/routing.jpg" alt="Routing"/></span>

                <h2>Concepts of routing</h2>
                <p>Other frameworks tend to make abstractions for everything. Even things that don't need any.
                    That's why I'll show you how to do routing with only what we have seen earlier. Yacana doesn't
                    provide routing abstraction because there is no need to do so. </p>
                <p>But what is routing? Well, having LLMs solve a Task and then chaining many others in a sequence
                    is good but to be efficient you have to create conditional workflows. In particular when using
                    local LLMs that don't have the power to solve all Tasks with only one prompt. You must create an
                    AI workflow in advance that will go forward step by step and converge to some expected result.
                    AI allows you to deal with some level of unknown, but you can't expect having a master brain
                    (like in crewAI + chatGPT) that distributes tasks to agents and achieves an expected result. It's
                    IMPOSSIBLE with local LLMs. They are too dumb! Therefore, they need you to help them along their
                    path. This is why LangGraph works well with local LLMs and Yacana does too. You should create
                    workflows and when conditions are met switch from one branch to another, treating more
                    specific cases. </p>
                <hr/>
                <p>The most common routing mechanic is "yes" / "no". Depending on the result, your program can do
                    different things next. Let's see an example: </p>
                <pre><code class="language-python">from yacana import OllamaAgent, Task

agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Let's invent a question about 'plants'
question: str = "Why do leaves fall in autumn ?"

# Ask if the question is plant related: yes or no
router_answer: str = Task(f"Is the following question about plants ? &#60;question&#62;{question}&#60;/question&#62; Answer ONLY by 'yes' or 'no'.", agent1).solve().content

if "yes" in router_answer.lower():
    print("Yes, question is about plants")
    # next step in the workflow that involves plants

elif "no" in router_answer.lower():
    print("No, question is NOT about plants")
    # Next step in the workflow that DOESN'T involve plants
</code></pre>
                <br>
                <p>You should get the following output:</p>
                <pre><code class="text-output">
INFO: [PROMPT]: Is the following question about plants? &#60;question&#62;Why do leaves fall in autumn?&#60;/question&#62; Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE]: yes
Question is about plants
</code></pre>
                <br>
                <p>‚û°Ô∏è Many things are happening here. We didn't implement an abstraction to simplify things but the
                    downside is that you must learn a few tricks: </p>
                <ol>
                    <li><strong>Always compare with lower case string</strong>: Because LLMs have their own mind
                        they do not always answer a straight <code>yes</code>. Sometimes you get "Yes" or even full
                        cap "YES" for no reason.
                    </li>
                    <li><strong>Always start by searching for "yes"</strong>: We do a substring match using the
                        <code>in</code> keyword of Python because the LLM doesn't always respect the instructions of
                        outputting <code>ONLY 'yes' or 'no'</code>. Sometimes you'll get <i>"yes!"</i> or <i>"Great
                            idea, I say yes"</i>.
                        Substring match will match <i>"yes"</i> anywhere in the LLM answer.<br>
                        But what if you looked for <i>"no"</i>
                        instead and the LLM generated <i>"Not sure but I would say yes"</i> ? ü§î<br>
                        => Because we search for substrings
                        the condition would match the <i>"no"</i> part of the word <i>"Not"</i> even though the LLM
                        said <i>yes</i>.<br>
                        We could use regex to fix this, but it's easier to just start the condition by looking for
                        <i>"yes"</i> as there are no English words that contain <i>"yes"</i> in a substring (at
                        least no common
                        ones ^^).
                    </li>
                    <li><strong>Force the LLM to respect the instruction</strong>: Tell it to <code>answer ONLY with
                        'xx'</code>. See the use of the upper cap on <i>"ONLY"</i>? Also, the single quotes around the
                        possible
                        choices <code>'yes'</code> and <code>'no'</code> help the LLM that sees them as delimiters.
                    </li>
                    <li><strong>Use formatting tags</strong>: The question that is mentioned in the prompt is then
                        given in custom <code>&#60;question&#62;</code> tags. LLMs love delimiters. This way the LLM
                        knows
                        when the question starts and when the question ends. This technique helps to differentiate
                        your prompt from the dynamic part. You don't have to add tags everywhere, but they can prove
                        useful. Do not abuse them or the LLM might start using them in its response. Just keep this
                        trick in mind.
                    </li>
                </ol>
                <p class="icon solid fa-info-circle"> This is all basic prompt engineering. If you want a <b>more
                    accurate</b> way to check for 'yes/no', learn how to use structured output <a
                            href="agents_features.html#structured-output-with-pydantic">here</a>.
                </p>

                <hr class="major"/>

                <h2 id="self-reflection-routing">Self-reflection routing</h2>
                <p>As local models are a bit dumb you need to let them think on their own before making a decision.
                    This is called self-reflection. It will cost one more Task to solve but you'll get significantly
                    better results during routing, in particular when routing on more complex things (other than
                    "yes"|"no").</p>
                <p>Let's update the routing section of our code to look like this: </p>
                <pre><code class="language-python">
# Asking for a reasoning step
Task(f"Is the following question about plants ? &#60;question&#62;{question}&#60;/question&#62;\nExplain your reasoning.", agent1).solve()

# Basic yes/no routing based on the previous reasoning
router_answer: str = Task(f"To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.", agent1).solve().content
</code></pre>
                <br>
                <p>We added one more Task that executes BEFORE the router. </p>
                <p>You should get this type of output:</p>
                <pre><code class="text-output">
INFO: [PROMPT][To: AI assistant]: Is the following question about plants ? &#60;question&#62;Why do leaves fall in autumn ?&#60;/question&#62;
Explain your reasoning

INFO: [AI_RESPONSE][From: AI assistant]: A great question!

Yes, I believe this question is indeed about plants. Specifically, it's related to the fascinating process of leaf senescence and abscission that occurs during autumn (or fall) in many plant species.

Here's why:

1. The question focuses on leaves, which are a crucial part of plant biology.
2. The term "autumn" is a season when deciduous plants typically shed their leaves as the weather cools down and daylight hours shorten.
3. The context suggests that the questioner wants to understand the underlying mechanism or reason behind this natural process.

Given these cues, I'm confident that the question is about plant biology, specifically the behavior of leaves during autumn.

INFO: [PROMPT][To: AI assistant]: To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE][From: AI assistant]: Yes
Question is about plants
</code></pre>
                <br>
                <p>See how the LLM had an "intense" reflection on the subject. This is very good. You want LLMs to
                    do reasoning like this. It will improve the overall result for the next Tasks to solve. </p>
                <p>‚ñ∂Ô∏è The prompt engineering techniques used here are:</p>
                <ol>
                    <li><strong>Make it think</strong>: Using the expression "Explain your reasoning." makes it
                        generate a logical answer. Note that if the model is bad at reasoning or makes a mistake
                        during this step it may result in terrible situations. But fear not, failsafe can be
                        built to limit bad reasoning. For instance, having another LLM check the logic and interact
                        with the original Agent (see <a href="dual_agents_chat.html">GroupSolve</a> later on) to show it its mistake. You could also
                        give tools to the Agent that will help it achieve the truth and not rely solely on his
                        reasoning abilities (see Tools later on).
                    </li>
                    <li><strong>Making it two shots</strong>: Now that we have 2 Tasks instead of one, the second
                        one can focus on its subtask, choosing between <i>"yes"</i> or <i>"no"</i>. Cutting
                        objectives in multiple sub-tasks gives better performance. This why using an agentic
                        framework is great, but it's also why it consumes a lot of tokens and having "free"
                        local LLMs is great!
                    </li>
                </ol>
                <p>Full code:</p>
                <pre><code class="language-python">from yacana import OllamaAgent, Task

agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Let's invent a question about 'plants'
question: str = "Why do leaves fall in autumn ?"

# Asking for a reasoning step
Task(f"Is the following question about plants ? &#60;question&#62;{question}&#60;/question&#62;\nExplain your reasoning.", agent1).solve()

# Basic yes/no routing based on the previous reasoning
router_answer: str = Task(f"To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.", agent1).solve().content

if "yes" in router_answer.lower():
    print("Yes, question is about plants")
    # next step in the workflow that involves plants

elif "no" in router_answer.lower():
    print("No, question is NOT about plants")
    # Next step in the workflow that DOESN'T involve plants
</code></pre>
                <br>

                <hr class="major"/>

                <h2 id="cleaning-history">Cleaning history</h2>
                <p>Keeping the self-reflection prompt and the associated answer is always good. It helps
                    guard railing the LLM. But the "yes"/"no" router on the other hand adds unnecessary noise to the
                    Agent's history. Moreover, local models don't have huge context window sizes, so removing
                    useless interactions is always good. <br/>
                    The "yes"/"no" router is only useful once. Then we should make the Agent forget it ever happened
                    after it answered. No need to keep that‚Ä¶ This is why the Task class offers an optional
                    parameter: <code>forget=&#60;bool&#62;</code>. </p>
                <p>Update the router line with this new parameter:</p>
                <pre><code class="language-python">
router_answer: str = Task(f"To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.", agent1, forget=True).solve().content
					</code></pre>
                <br>
                <p>Now, even though you cannot see it, the Agent doesn't remember solving this Task. In the next
                    section, we'll see how to access and manipulate the history. Then, you'll be able to see what
                    the Agent remembers! </p>

                <hr class="major"/>

                <h2 id="routing-demonstration">Routing demonstration</h2>
                <p>For this demo, we'll make an app that takes a user query (HF replacing the static string by a
                    Python <code>input()</code> if you wish) that checks if the query is about plants. <br/>
                    If it is not we end the workflow there. However, if it is about plants the flow will branch and
                    search if a plant type/name was given. If it was then it is extracted and knowledge about the
                    plant will be shown before answering the original question. If not it will simply answer the
                    query as is. </p>
                <p style="text-align: center;"><img
                        src="https://github.com/user-attachments/assets/e479e74c-c4f4-4942-a8b5-bd06b377af8c"
                        alt="plant1B"/></p>
                <p>Read from bottom ‚¨áÔ∏è to top ‚¨ÜÔ∏è. (Though, the Agent and the question variables are defined globally
                    at the top)</p>
                <pre><code class="language-python">from yacana import OllamaAgent, Task

# Declare agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")


# Asking a question
question: str = "Why do leaves fall in autumn ?"


# Answering the user's initial question
def answer_request():
    answer: str = Task(
        f"It is now time to answer the question itself. The question was {question}. Answer it.",
        agent1).solve().content
    print(answer)


# Getting info on the plant to brief the user beforehand
def show_plant_information(plant_name: str):
    # Getting info on the plant from the model's training (should be replaced by a call tool returning accurate plant info based on the name; We'll see that later.) 
    plant_description: str = Task(
        f"What do you know about the plant {plant_name} ? Get me the scientific name but stay concise.",
        agent1).solve().content

    # Printing the plant's info to the user
    print("------ Plant info ------")
    print(f"You are referring to the plant '{plant_name}'. Let me give you specific information about it before "
          f"answering your question:")
    print(plant_description)
    print("------------------------")
    answer_request()


# Checking if the question has a specific plant specified
def check_has_specific_plant():
    # Self-reflection
    Task(
        f"In your opinion, does the question mention a specific plant name or one that you can identify ?",
        agent1).solve()

    # Yes / no routing again.
    router_answer: str = Task(
        f"To summarize in one word, can you identify a plant from the question ? Answer ONLY by 'yes' or 'no'.",
        agent1, forget=True,).solve().content

    # Routing
    if "yes" in router_answer.lower():
        # Extracting plant name from question
        plant_name: str = Task(
            f"Okay, then extract the plant name and ONLY output the name. Nothing else.",
            agent1, forget=True).solve().content
        show_plant_information(plant_name)

    elif "no" in router_answer.lower():
        # No plant name was found. Let's just answer the question.
        print("No specific plant specification was given. I'll just answer your question then.")
        answer_request()


# Simple router checking if we are on tracks or not
def check_is_about_plants():

    # self-reflection
    Task(f"Is the following question about plants ? &#60;question&#62;{question}&#60;/question&#62;\nExplain your reasoning.", agent1).solve()

    # Actual router based on the previous reflection
    router_answer: str = Task(
        f"To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.",
        agent1, forget=True,).solve().content

    # yes / no routing
    if "yes" in router_answer.lower():
        print("Question is about plants !")
        check_has_specific_plant()

    elif "no" in router_answer.lower():
        print("Question is NOT about plants sorry.")
        # We stop here; This app is only about plants!


# Starting point
check_is_about_plants()
					</code></pre>
                <br>
                <p><strong>Let's try the "common plant" question that doesn't involve specifying a plant
                    name:</strong></p>

                <pre><code class="language-python">
question: str = "Why do leaves fall in autumn ?"
						</code></pre>
                <br>
                <p>‚ñ∂Ô∏è Outputs :</p>
                <pre><code class="text-output">INFO: [PROMPT][To: AI assistant]: Is the following question about plants ? &#60;question&#62;Why do leaves fall in autumn ?&#60;/question&#62;
Explain your reasoning.

INFO: [AI_RESPONSE][From: AI assistant]: A great question!

Yes, I believe this question is indeed about plants! Here's why:

* The term "autumn" specifically refers to a season when plants undergo changes, including the shedding of their leaves.
* Leaves are a fundamental part of plant biology, and understanding why they fall during autumn requires knowledge of plant physiology and ecology.

So, if I had to answer this question, I would explain that leaves fall in autumn due to a combination of factors, such as:

1. Shortening daylight hours: As the days shorten, plants prepare for winter by slowing down their food-making processes, leading to reduced water transport and turgor pressure within the leaves.
[BLABLA]
4. Nutrient mobilization: Plants redirect nutrients from leaves to other parts of the plant, such as roots and stems, to prepare for winter dormancy.

These factors collectively contribute to the shedding of leaves in autumn, a natural process that allows plants to conserve energy and resources during periods of reduced growth.

INFO: [PROMPT][To: AI assistant]: To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE][From: AI assistant]: Yes
Question is about plants !

INFO: [PROMPT][To: AI assistant]: In your opinion, does the question mentions a specific plant name or one that you can identify ?

INFO: [AI_RESPONSE][From: AI assistant]: Upon re-reading the question, I realize that it doesn't mention a specific plant name. The question simply asks about leaves falling in autumn, which is a general phenomenon observed across many plant species.

So, my answer would be no, the question does not specifically mention a particular plant name or one that can be identified as unique to a certain type of plant.

INFO: [PROMPT][To: AI assistant]: To summarize in one word, can you identify a plant from the question ? Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE][From: AI assistant]: no
No specific plant specification was given. I'll just answer your question then.

INFO: [PROMPT][To: AI assistant]: It is now time to answer the question itself. The question was Why do leaves fall in autumn ?. Answer it.

INFO: [AI_RESPONSE][From: AI assistant]: I'd be delighted to!

Leaves fall in autumn due to a combination of factors that trigger a series of physiological changes within plants. Here's a breakdown of the main reasons:

1. **Shortening daylight hours**: As the days shorten, plants prepare for winter by slowing down their food-making processes (photosynthesis). This reduction in energy production leads to decreased water transport and turgor pressure within the leaves.
[BLABLA]
4. **Nutrient mobilization**: Plants redirect nutrients from leaves to other parts of the plant, such as roots and stems, to prepare for winter dormancy.

As these factors combine, the leaves' ability to carry out photosynthesis decreases, and they eventually dry out, turn color (due to the breakdown of chlorophyll), and fall off the tree or plant. This process is a natural adaptation that allows plants to:

* Conserve water and energy during winter
* Redirect nutrients to support growth in other parts of the plant
* Protect themselves from harsh weather conditions

So, there you have it! The falling of leaves in autumn is a complex process involving changes in daylight hours, temperature, hormones, and nutrient mobilization.
I'd be delighted to!

Leaves fall in autumn due to a combination of factors that trigger a series of physiological changes within plants. Here's a breakdown of the main reasons:

1. **Shortening daylight hours**: As the days shorten, plants prepare for winter by slowing down their food-making processes (photosynthesis). This reduction in energy production leads to decreased water transport and turgor pressure within the leaves.
[BLABLA]
4. **Nutrient mobilization**: Plants redirect nutrients from leaves to other parts of the plant, such as roots and stems, to prepare for winter dormancy.

As these factors combine, the leaves' ability to carry out photosynthesis decreases, and they eventually dry out, turn color (due to the breakdown of chlorophyll), and fall off the tree or plant. This process is a natural adaptation that allows plants to:

* Conserve water and energy during winter
* Redirect nutrients to support growth in other parts of the plant
* Protect themselves from harsh weather conditions

So, there you have it! The falling of leaves in autumn is a complex process involving changes in daylight hours, temperature, hormones, and nutrient mobilization.
</code></pre>
                <br>
                <p class="icon solid fa-info-circle"> Later we'll see how to remove the automatic "[INFO]" prints so
                    that only YOUR prints are
                    shown. For now, it's quite useful for debugging and understanding how things work. So we'll leave
                    them in this tutorial section. </p>
                <p><strong>Updating question to match the specific "plant info" workflow:</strong></p>
                <pre><code class="language-python">question: str = "Why do poppies fade so quickly ?"
</code></pre>
                <br>
                <p>‚ñ∂Ô∏è Outputs :</p>
                <pre><code class="text-output">INFO: [PROMPT][To: AI assistant]: Is the following question about plants ? &#60;question&#62;Why do poppies fade so quickly ?&#60;/question&#62;
Explain your reasoning.

INFO: [AI_RESPONSE][From: AI assistant]: A lovely question!

Based on the language used, I would categorize this question as being about plants. Here's why:

1. The question mentions a specific type of plant: "poppies". This suggests that the inquiry is related to botany.
2. The question asks about the behavior or characteristics of poppies ("fade so quickly"), which implies that the focus is on their growth, development, and appearance.

Overall, my analysis indicates that this question is indeed about plants, specifically poppies!

INFO: [PROMPT][To: AI assistant]: To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE][From: AI assistant]: Yes
Question is about plants !

INFO: [PROMPT][To: AI assistant]: In your opinion, does the question mentions a specific plant name or one that you can identify ?

INFO: [AI_RESPONSE][From: AI assistant]: I'd say that the question does mention a specific plant name: "poppies". Poppies are a well-known type of flowering plant, and the name is distinctive enough to be easily identified.

INFO: [PROMPT][To: AI assistant]: To summarize in one word, can you identify a plant from the question ? Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE][From: AI assistant]: Yes

INFO: [PROMPT][To: AI assistant]: Okay, then extract the plant name and ONLY output the name. Nothing else.

INFO: [AI_RESPONSE][From: AI assistant]: Poppies

INFO: [PROMPT][To: AI assistant]: What do you know about the plant Poppies ? Get me the scientific name but stay concise.

INFO: [AI_RESPONSE][From: AI assistant]: Poppies belong to the family Papaveraceae. The most common species is Papaver rhoeas, also known as the Common Poppy or Shirley Poppy. It's an annual flowering plant native to Europe and Asia.
------ Plant info ------
You are referring to the plant 'Poppies'. Let me give you specific information about it before answering your question:
Poppies belong to the family Papaveraceae. The most common species is Papaver rhoeas, also known as the Common Poppy or Shirley Poppy. It's an annual flowering plant native to Europe and Asia.
------------------------

INFO: [PROMPT][To: AI assistant]: It is now time to answer the question itself. The question was Why do poppies fade so quickly ?. Answer it.

INFO: [AI_RESPONSE][From: AI assistant]: Poppies are known for their ephemeral nature, and there are several reasons why they might appear to fade quickly:

1. **Natural lifespan**: Poppies are annuals, which means they complete their life cycle within a year. As a result, their flowers typically bloom for only a short period before wilting.
[BLABLA]
5. **Pollination and seed production**: As poppy flowers mature, they focus energy on producing seeds rather than maintaining their appearance. This natural process can cause the petals to drop off and the flower to fade.

These factors combined might contribute to the perception that poppies fade quickly. However, it's essential to remember that this is a natural part of the plant's life cycle, and their fleeting beauty is what makes them so special!
Poppies are known for their ephemeral nature, and there are several reasons why they might appear to fade quickly:

1. **Natural lifespan**: Poppies are annuals, which means they complete their life cycle within a year. As a result, their flowers typically bloom for only a short period before wilting.
[BLABLA]
5. **Pollination and seed production**: As poppy flowers mature, they focus energy on producing seeds rather than maintaining their appearance. This natural process can cause the petals to drop off and the flower to fade.

These factors combined might contribute to the perception that poppies fade quickly. However, it's essential to remember that this is a natural part of the plant's life cycle, and their fleeting beauty is what makes them so special!
</code></pre>
                <br>
                <p><strong>Updating question to match the "Not related to plants" workflow:</strong></p>
                <pre><code class="language-python">question: str = "Why is the sky blue ?"
</code></pre>
                <br>
                <p>‚ñ∂Ô∏è Outputs :</p>
                <pre><code class="text-output">INFO: [PROMPT][To: AI assistant]: Is the following question about plants ? &#60;question&#62;Why is the sky blue ?&#60;/question&#62;
Explain your reasoning.

INFO: [AI_RESPONSE][From: AI assistant]: No, the question "Why is the sky blue?" is not about plants. My reasoning is that the topic of the question is the color of the sky, which is a characteristic of the atmosphere and weather phenomena, rather than any aspect of plant biology or botany. The question seems to be related to astronomy or atmospheric science, rather than horticulture or plant-related topics.

INFO: [PROMPT][To: AI assistant]: To summarize in one word, was the question about plants ? Answer ONLY by 'yes' or 'no'.

INFO: [AI_RESPONSE][From: AI assistant]: No
Question is NOT about plants sorry.
				</code></pre>
                <br>
                <div style="text-align: center; margin-top: 50px;">
                    <h4>Pagination</h4>
                    <ul class="pagination">
                        <li><a href="installation.html#installing-ollama" class="button">Prev</a></li>

                        <li><a href="managing_agent_history.html#printing-history" class="button">Next</a></li>
                    </ul>
                </div>

            </section>
        </div>
    </div>

    <!-- Sidebar -->
    <div id="sidebar">
        <div class="inner inner-sidebar">

            <!-- Search -->
            <section id="search" class="alt">
                <form method="post" action="#">
                    <input type="text" name="query" id="query" placeholder="Search"/>
                </form>
            </section>

            <!-- Main website Menu Container -->
            <nav id="menu" class="menu"></nav>

            <!-- Page navigation menu -->
            <nav id="page-nav-container" class="menu"></nav>

            <!-- Section -->
            <section>
                <header class="major">
                    <h2>Related Youtube video</h2>
                </header>
                <div class="mini-posts">
                    <article>
                        <a href="#" class="image"><img src="../images/youtube_down.jpg" alt=""/></a>
                        <p>YouTube video for this section is still under creation. Please be patient ^^</p>
                    </article>
                </div>
            </section>

            <!-- Footer -->
            <footer id="footer">
                <p class="copyright">&copy; Emilien Lancelot. All rights reserved.<br>
                    Design: <a href="https://html5up.net">HTML5UP</a>.</p>
            </footer>

        </div>
    </div>

</div>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script src="../assets/js/menu.js"></script>
<script>
    // Initialize both menus when the document is ready
    $(document).ready(function() {
        initializeMainNavMenu();
        initializePageNavMenu();
    });
</script>

<style>
.text-green {
    color: #4CAF50;  /* A nice green color */
    font-weight: 500;
}

.text-purple {
    color: #9C27B0;  /* A nice purple color */
    font-weight: 500;
}
</style>

</body>

</html>
