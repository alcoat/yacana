<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Yacana - Agents & Tasks</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="../assets/css/main.css"/>
    <link rel="stylesheet" href="../assets/css/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/monokai.min.css">
    <link rel="stylesheet" href="../assets/css/foldgutter.min.css">
    <link rel="stylesheet" href="../assets/css/codemirror-custom.css">
    <link rel="stylesheet" href="../assets/css/zenburn.min.css">
    <script src="../assets/js/codemirror.min.js"></script>
    <script src="../assets/js/python.min.js"></script>
    <script src="../assets/js/foldcode.min.js"></script>
    <script src="../assets/js/foldgutter.min.js"></script>
    <script src="../assets/js/brace-fold.min.js"></script>
    <script src="../assets/js/codemirror-custom.js"></script>
</head>

<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">
        <div class="inner">

            <!-- Header -->
            <header id="header">
                <a href="../index.html" class="logo"><strong>Yacana</strong>, powering open source LLMs</a>
                <ul class="icons">
                    <li><a href="https://x.com/RSoftwares_ofc" class="icon brands fa-twitter"><span class="label">Twitter</span></a>
                    </li>
                    <li><a href="https://medium.com/@docteur_rs" class="icon brands fa-medium-m"><span class="label">Medium</span></a>
                    </li>
                    <li><a href="https://www.youtube.com/channel/UCvi7R0CRmtxhWOVw62XteTw"
                           class="icon brands fa-youtube"><span class="label">Medium</span></a></li>
                    <li><a href="https://github.com/rememberSoftwares/yacana" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                </ul>
            </header>

            <!-- Content -->
            <section>
                <header class="main">
                    <h1 id="creating-an-agent">II. Agents & Tasks</h1>
                </header>

                <span class="image main"><img src="../images/agents_and_tasks.jpg"
                                              alt="Creating Agents and Tasks to solve"/></span>


                <h2>Creating an Agent</h2>

                <p>Now that you have an Ollama server running and Yacana installed let's create our first agent!</p>
                <p>Create a Python file with this content:</p>
                <pre><code class="language-python">
from yacana import OllamaAgent

agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:11434")
					</code></pre>
                <br>
                <p>The <code>OllamaAgent()</code> class takes...</p>
                <span class="icon solid fa-chevron-right"> 2 <u>mandatory</u> parameters:</span>
                <ol>
                    <li><strong>The agent name:</strong> Choose something short about the agent's global focus</li>
                    <li><strong>A model name:</strong> The Ollama model that this Agent will use. You may have
                        multiple Agents running different models. Some models are better suited for some specific
                        jobs so it can be interesting to mix LLM models. Use <code>ollama list</code> to list the
                        models you have downloaded.
                    </li>
                </ol>
                <span class="icon solid fa-chevron-right"> many <u>optional</u> parameters that we will discover in
						this tutorial. Here we
						can see 2 of them:</span>
                <ol>
                    <li><strong>The system prompt:</strong> Helps define the personality of the Agent.</li>
                    <li><strong>The endpoint:</strong> The URL of your Ollama instance. It points by default to your
                        localhost and on the Ollama default port. If you are using Ollama on your computer you can
                        remove this optional parameter and the default value will be used.
                    </li>
                </ol>

                <hr class="major"/>

                <h2 id="basic-roleplay">Basic roleplay</h2>
                <p>This framework is not meant for basic roleplay. However, for people starting their journey in the
                    realm of AI and for debugging purposes, we added a simple chat system. Add this new line to test
                    it :</p>
                <pre><code class="language-python">
agent1.simple_chat()
					</code></pre>
                <br>
                <p>When running this python file you should enter a chat with the LLM. The Agent keeps track of the
                    history so that it can answer using past information.</p>
                <pre><code class="text-output">
$ python3 simple_chat_demo.py
					</code></pre>
                <br>
                <span>▶️ Output:</span>
                <pre><code class="text-output">
Type 'quit' then enter to exit.
> hey
Hey! It's nice to meet you. Is there something I can help you with, or would you like to chat about something in particular? I'm here to assist you with any questions or topics you'd like to discuss.
>
					</code></pre>
                <br>
                <p>Let's change the <strong>system prompt</strong> and have some fun!</p>
                <pre><code class="language-python">
agent1 = OllamaAgent("Pirate", "llama3.1:8b", system_prompt="You are a pirate", endpoint="http://127.0.0.1:11434")
					</code></pre>
                <br>
                <span>▶️ Output:</span>
                <pre><code class="text-output">
Type 'quit' then enter to exit.
> hey
Arrrr, shiver me timbers! What be bringin' ye to these fair waters? Are ye lookin' fer a swashbucklin' adventure or just passin' through?
> Searching for the tresor of red beard any idea where it's hidden ?
Red Beard's treasure, ye say? (puffs on pipe) Well, I be knowin' a thing or two about that scurvy dog and his loot. But, I'll only be tellin' ye if ye be willin' to share yer own booty... of information! (winks)
					</code></pre>
                <br>
                <hr>
                <p>Complete section code:</p>
                <pre><code class="language-python">
from yacana import OllamaAgent

agent1 = OllamaAgent("Pirate", "llama3.1:8b", system_prompt="You are a pirate", endpoint="http://127.0.0.1:11434")
agent1.simple_chat()
					</code></pre>
                <br>
                <p>⚠️From now on, for clarity, we will not set the <em>endpoint</em> attribute anymore for clarity and
                    will resort
                    to the defaults. If your LLM is not served by Ollama or isn't on your localhost you should
                    continue setting this value.</p>

                <hr class="major"/>

                <h2 id="creating-tasks">Creating Tasks</h2>
                <p>The whole concept of the framework lies here. If you understand this following section then you
                    have mastered 80% of Yacana's building principle. Like in LangGraph, where you create nodes that
                    you link together, Yacana has a Task() class which takes as argument a task to solve. There are
                    no hardcoded links between the Tasks so it's easy to refactor and move things around. The
                    important concept to grasp here is that through these Tasks you will give instructions to the
                    LLM in a way that the result must be computable. Meaning instructions must be clear and the
                    prompt to use must reflect that. It's a Task, it's a job, it's something that needs solving but
                    written like it is given as an order! Let's see some examples :</p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

# First, let's make a basic AI agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Now we create a task and assign the agent1 to the task
task1 = Task(f"Solve the equation 2 + 2 and output the result", agent1)

# For something to happen, you must call the .solve() method on your task.
task1.solve()
					</code></pre>
                <br>
                <p>What's happening above?</p>
                <ul>
                    <li>First, we instantiated an Agent with the <code>llama3.1:8b</code> model. You might need to
                        update that depending on what LLM you downloaded from Ollama ;
                    </li>
                    <li>Second, we instantiated a Task ;</li>
                    <li>Third, we asked that the Task be solved ;</li>
                </ul>
                <p class="icon solid fa-info-circle"> For easing the learning curve the default logging level is
                    INFO. It will show what is going on in Yacana. Note that NOT ALL prompts are shown.</p>
                <p>The output should look like this:</p>
                <pre><code class="text-output">
INFO: [PROMPT][To: AI assistant]: Solve the equation 2 + 2 and output the result

INFO: [AI_RESPONSE][From: AI assistant]: The answer to the equation 2 + 2 is... (drumroll please)... 4!
							
So, the result of solving the equation 2 + 2 is indeed 4.
							</code></pre>
                <br>
                <p>If your terminal is working normally <span class="text-green">you should see the task's prompts in green and starting with
                    the '[PROMPT]' string.</span> <span class="text-purple">The LLM's answer should appear purple and start with the [AI_RESPONSE]
                    string.</span></p>
                <h4 id="task-parameters">Task parameters</h4>
                <p>The Task class takes 2 mandatory parameters:</p>
                <ul>
                    <li>The prompt: It is the task to be solved. Use imperative language, be precise, and ask for
                        step-by-step thinking for complex Tasks and expected outputs if needed.
                    </li>
                    <li>The agent that will be assigned to this task. The agent will be in charge of
                        solving the task.
                    </li>
                </ul>
                <p class="icon solid fa-info-circle"> Many other parameters can be given to a Task. We will see some
                    of them in the following sections of this tutorial. But you can already check out the Task class
                    documentation.</p> @todo link to documentation
                <h4 id="in-what-way-is-this-disruptive-compared-to-other-frameworks">In what way is this disruptive
                    compared to other frameworks?</h4>
                <p>In the above code snippet, we assigned the agent to the Task. So it's the Task that leads the
                    direction that the AI takes. In most other frameworks it's the opposite. You assign work to an
                    existing agent. This reversed way allows to have fine-grained control on each resolution step
                    as the LLM only follows breadcrumbs (the tasks). The pattern will become even more obvious as we
                    get to the Tool section of this tutorial. As you'll see the Tools are also assigned at the Task
                    level and not to the Agent directly.</p>
                <p>Compared with LangGraph, we cannot generate a call graph as an image because we don't
                    bind the tasks together explicitly. However, Yacana's way gives more flexibility and allows a
                    hierarchical programming way of scheduling the tasks and keeping control of the flow. It also
                    allows creating new Tasks dynamically if the need arises. You shall rely on your programming
                    skills and good OOP to have a clean code and good Task ordering. Yacana will never propose
                    any hardlinked code or flat configurations.</p>

                <hr class="major"/>

                <h2 id="getting-the-result-of-a-task">Getting the result of a Task</h2>
                <p>Although the logs appear in the terminal's standard output, we still need to extract the LLM's response to the Task in order to use it.<br>
                    Getting the string message out of it is quite easy as the <code>.solve()</code> method returns a <code></code>Message()</code> class.<br>
                    Maybe you are thinking &quot;Ho nooo, another class to deal with&quot;. Well, let me tell you
                    that it's always better to have an OOP class than some semi-random Python dictionary where
                    you'll forget what keys it contains in a matter of minutes. Also, the Message class is very
                    straightforward. It exposes a <code>content</code> attribute. Update the current code to look
                    like this:</p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task, Message

# First, let's make a basic AI agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Now we create a task and assign the agent1 to the task
task1 = Task(f"Solve the equation 2 + 2 and output the result", agent1)

# So that something actually happens you must call the .solve() method on your task
my_message: Message = task1.solve()

# Printing the LLM's response
print(f"The AI response to our task is : {my_message.content}")
						</code></pre>
                <br>
                <p>There you go! Give it a try.</p>
                <p class="icon solid fa-info-circle"> Note that we used duck typing to postfix all variables
                    declaration with their type <code>my_message: Message</code>. Yacana's source code is entirely
                    duck-typed so that your IDE always knows what type it's dealing with and proposes the best
                    methods and arguments. We recommend that you do the same as it's the industry's best standards.
                </p>
                <hr>
                <p>Don't like having 100 lines of code for something simple? Then chain them all!</p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

# First, let's make a basic AI agent
agent1 = OllamaAgent("AI assistant", "llama3.1:8b", system_prompt="You are a helpful AI assistant")

# Creating the task, solving it, extracting the result
result: str = Task(f'Solve the equation 2 + 2 and output the result', agent1).solve().content
# Print the result
print(f"The AI response to our task is: {result}")
					</code></pre>

                <hr class="major"/>

                <h2 id="chaining-tasks">Chaining Tasks</h2>
                <p>Agents keep track of the History of what they did (aka, all the Tasks they solved). So just call a second
                    Task and assign the same Agent. For instance, let's multiply by 2 the result of the initial
                    Task. Append this to our current script:</p>
                <pre><code class="language-python">
task2_result: str = Task(f'Multiply by 2 our previous result', agent1).solve().content
print(f"The AI response to our second task is : {task2_result}")
					</code></pre>
                <br>
                <p>You should get:</p>
                <pre><code class="text-output">
The AI response to our task is: If we multiply the previous result of 4 by 2, we get:
							
8
					</code></pre>
                <br>
                <p class="icon solid fa-info-circle"> Without tools this only relies on the LLM's ability to do the
                    maths and is dependent on its training.</p>
                <hr>
                <p>See? The assigned Agent remembered that it had solved Task1 previously and used this
                    information to solve the second task.<br>
                    You can chain as many Tasks as you need. You can build anything now!</p>

                <hr class="major"/>

                <h2 id="logging-levels">Logging levels</h2>
                <p>As entering the AI landscape can get a bit hairy we decided to leave the INFO log level by
                    default. This allows to log to the standard output all the requests made to the LLM.<br>
                    Note that NOT everything of Yacana's internal magic appears in these logs. We don't show
                    everything because many time-traveling things are going around inside the history of an Agent and
                    printing a log at the time it is generated wouldn't always make sense.<br>
                    However, we try to log a maximum of information to help you understand what is happening
                    internally and allow you to tweak your prompts accordingly.</p>
                <p>Nonetheless, you are the master of what is logged and what isn't. You cannot let Yacana log
                    anything while working with an app in production.<br>
                    There are 5 levels of logs:</p>
                <ol>
                    <li><code>"DEBUG"</code></li>
                    <li><code>"INFO"</code> <span class="icon solid fa-chevron-left"></span> Default</li>
                    <li><code>"WARNING"</code></li>
                    <li><code>"ERROR"</code></li>
                    <li><code>"CRITICAL"</code></li>
                    <li><code>None</code> <span class="icon solid fa-chevron-left"></span> No logs</li>
                </ol>
                <p>To configure the log simply add this line at the start of your program:</p>
                <pre><code class="language-python">
from yacana import LoggerManager

LoggerManager.set_log_level("INFO")
					</code></pre>
                <br>
                <p class="icon solid fa-info-circle">Note that Yacana utilizes the Python logging package. This
                    means that setting the level to &quot;DEBUG&quot; will print other libraries' logs on the debug
                    level too.</p>
                <p>If you need a library to stop spamming, you can try the following:</p>
                <pre><code class="language-python">
from yacana import LoggerManager

LoggerManager.set_library_log_level("httpx", "WARNING")
					</code></pre>
                <br>
                <p>The above example sets the logging level of the network httpx library to warning, thus reducing
                    the log spamming.</p>

                <hr class="major"/>

                <h2 id="configuring-llms-settings">Configuring LLM's settings</h2>
                <p>For advanced users, Yacana provides a way to tweak the LLM runtime behavior!<br>
                    For instance, lowering the <code>temperature</code> setting makes the model less creative in its
                    responses. On the contrary, raising this setting will make the LLM more chatty and creative.<br>
                    Yacana provides you with a class that exposes all the possible LLM properties. Also if you need a
                    good explanation for each of them I would
                    recommend the <a href="https://youtu.be/QfFRNF5AhME?si=lpSYUq2WoidYiqzP">excellent video</a>
                    Matt Williams did on this subject.</p>
                <p class="icon solid fa-info-circle"> These settings are set at the Agent level so that you can have
                    the same underlying model used by two separate agents and have them behave differently.</p>
                <p class="icon solid fa-info-circle"> The <a href="">OllamaModelSettings</a> class is tailored for the Ollama backend. 
                    You can use <a href="">OpenAiModelSettings</a> to configure non-Ollama LLMs with their own set of available settings.</p>
                <p>We use the <a href="">OllamaModelSettings</a> class to configure the settings we need.</p>
                <p>For example, let's lower the temperature of an Agent to 0.4:</p>
                <pre><code class="language-python">
from yacana import OllamaModelSettings, OllamaAgent

ms = OllamaModelSettings(temperature=0.4)

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b", model_settings=ms)
					</code></pre>
                <br>
                <p>If you're wondering what the default values of these are when not set. Well, Ollama sets the
                    defaults for you. They can also be overridden in the Model config file (looks like a dockerfile
                    but for LLMs) and finally, you can set them through Yacana during runtime.</p>
                <p>A good way to show how this can have a real impact on the output is by setting the
                    <code>num_predict</code> parameter. This one allows control of how many tokens should be
                    generated by the LLM. Let's make the same Task twice but with different <code>num_predict</code>
                    values:
                </p>
                <pre><code class="language-python">
from yacana import OllamaModelSettings, OllamaAgent, Task

# Setting temperature and max token to 100
ms = OllamaModelSettings(temperature=0.4, num_predict=100)

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b", model_settings=ms)
Task("Why is the sky blue ?", agent1).solve()

print("-------------------")

# Settings max token to 15
ms = OllamaModelSettings(num_predict=15)

agent2 = OllamaAgent("Ai assistant", "llama3.1:8b", model_settings=ms)
Task("Why is the sky blue ?", agent2).solve()
					</code></pre>
                <br>
                <span>▶️ Output:</span>
                <pre><code class="text-output">
INFO: [PROMPT]: Why is the sky blue ?

INFO: [AI_RESPONSE]: The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. Here's what happens:

1. **Sunlight**: When sunlight enters Earth's atmosphere, it contains all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).
2. **Molecules**: The atmosphere is made up of tiny molecules of gases like nitrogen (N2) and oxygen (O2). These molecules are much smaller than

-------------------

INFO: [PROMPT]: Why is the sky blue ?

INFO: [AI_RESPONSE]: The sky appears blue because of a phenomenon called Rayleigh scattering, named after
					</code></pre>
                <br>
                <p>As you can see above the two agents didn't output the same number of tokens.</p>
                <div style="text-align: center; margin-top: 50px;">
                    <h4>Pagination</h4>
                    <ul class="pagination">
                        <li><a href="installation.html#installing-ollama" class="button">Prev</a></li>

                        <li><a href="routing.html#concepts-of-routing" class="button">Next</a></li>
                    </ul>
                </div>
            </section>
        </div>
    </div>

    <!-- Sidebar -->
    <div id="sidebar">
        <div class="inner">

            <!-- Search -->
            <section id="search" class="alt">
                <form method="post" action="#">
                    <input type="text" name="query" id="query" placeholder="Search"/>
                </form>
            </section>

            <!-- Menu Container -->
            <div id="menu-container"></div>

            <!-- Section -->
            <section>
                <header class="major">
                    <h2>Related Youtube video</h2>
                </header>
                <div class="mini-posts">
                    <article>
                        <a href="#" class="image"><img src="../images/youtube_down.jpg" alt=""/></a>
                        <p>Youtube video for this section is still under creation. Please be patient ^^</p>
                    </article>
                </div>
            </section>

            <!-- Section -->
            <section>
                <nav class="menu">
                    <header class="major">
                        <h2>Page menu</h2>
                    </header>
                    <ul>
                        <li><a href="agents_and_tasks.html#creating-an-agent">Creating an Agent</a></li>
                        <li><a href="agents_and_tasks.html#basic-roleplay">Basic roleplay</a></li>
                        <li><a href="agents_and_tasks.html#creating-tasks">Creating Tasks</a></li>
                        <li><a href="agents_and_tasks.html#getting-the-result-of-a-task">Getting the result of a Task</a></li>
                        <li><a href="agents_and_tasks.html#chaining-tasks">Chaining Tasks</a></li>
                        <li><a href="agents_and_tasks.html#logging-levels">Logging levels</a></li>
                        <li><a href="agents_and_tasks.html#configuring-llms-settings">Configuring LLM's settings</a></li>
                    </ul>
                </nav>
            </section>

            <!-- Footer -->
            <footer id="footer">
                <p class="copyright">&copy; Emilien Lancelot. All rights reserved.<br>
                    Design: <a href="https://html5up.net">HTML5UP</a>.</p>
            </footer>

        </div>
    </div>

</div>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script src="../assets/js/menu.js"></script>
<script>
    // Initialize the menu when the document is ready
    $(document).ready(function() {
        initializeMenu();
    });
</script>

<style>
.text-green {
    color: #4CAF50;  /* A nice green color */
    font-weight: 500;
}

.text-purple {
    color: #9C27B0;  /* A nice purple color */
    font-weight: 500;
}
</style>

</body>

</html>
