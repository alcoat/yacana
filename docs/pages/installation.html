<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Yacana - Installation</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="../assets/css/main.css"/>
    <link rel="stylesheet" href="../assets/css/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/monokai.min.css">
    <link rel="stylesheet" href="../assets/css/foldgutter.min.css">
    <link rel="stylesheet" href="../assets/css/codemirror-custom.css">
    <link rel="stylesheet" href="../assets/css/zenburn.min.css">
    <link rel="shortcut icon" href="../images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="../images/favicon.ico" type="image/x-icon">
    <script src="../assets/js/codemirror.min.js"></script>
    <script src="../assets/js/python.min.js"></script>
    <script src="../assets/js/json-lint.min.js"></script>
    <script src="../assets/js/foldcode.min.js"></script>
    <script src="../assets/js/foldgutter.min.js"></script>
    <script src="../assets/js/brace-fold.min.js"></script>
    <script src="../assets/js/codemirror-custom.js"></script>
</head>

<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">
        <div class="inner">

            <!-- Header -->
            <header id="header">
                <a href="../index.html" class="logo"><strong>Yacana</strong>, powering open source LLMs</a>
                <ul class="icons">
                    <li><a href="https://x.com/RSoftwares_ofc" class="icon brands fa-twitter"><span class="label">Twitter</span></a>
                    </li>
                    <li><a href="https://medium.com/@docteur_rs" class="icon brands fa-medium-m"><span class="label">Medium</span></a>
                    </li>
                    <li><a href="https://www.youtube.com/channel/UCvi7R0CRmtxhWOVw62XteTw"
                           class="icon brands fa-youtube"><span class="label">Medium</span></a></li>
                    <li><a href="https://github.com/rememberSoftwares/yacana" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                </ul>
            </header>

            <!-- Content -->
            <section>
                <header class="main">
                    <h1 id="installing-ollama">I. Installation</h1>
                </header>

                <span class="image main"><img src="../images/installation.jpg"
                                              alt="Yacana and Ollama installation"/></span>


                <h2>Installing Ollama</h2>
                <p style="text-align: center;"><img
                        src="https://github.com/user-attachments/assets/f3c45d0e-efca-4853-8237-3e56d90e1747"
                        alt="image"></p>
                <p>
                    Yacana was initially designed to work with <strong>Ollama</strong>, but now also supports any
                    OpenAI-compatible endpoints like <strong>ChatGPT</strong> or <strong>VLLM</strong>.<br>
                    This documentation serves as a comprehensive tutorial, guiding you through Yacana's features step by step.
                    Since the programming API is identical across all inference servers, we'll primarily demonstrate examples using the Ollama agent.
                    You can easily adapt any code snippets to work with your preferred inference server by simply <a href="other_inference_servers.html">swapping the agent type</a>.
                    Let's start by installing Ollama on your computer, it's one of the simplest inference servers to set up.
                </p>
                <p>
                    Click <a href="https://ollama.com/download">here</a> to get the latest release.<br>
                </p>
                <br>
                <span>Ollama is:</span>
                <ul>
                    <li>Compatible with all operating systems Windows/Mac/Linux ;</li>
                    <li>Installed in seconds using one command ;</li>
                    <li>Has a great CLi that even a 4-year-old can use to download models ;</li>
                    <li>Has tons of tutorials out there if you run into any trouble ;</li>
                </ul>
                <p class="icon solid fa-info-circle"> You can connect Yacana to a remote Ollama instance. Read
                    forward.</p>

                <hr class="major"/>

                <h2 id="choosing-an-llm-model">Choosing an LLM model</h2>
                <h4>Choosing a model that fits</h4>
                <p>
                    After Ollama is installed you can browse the list of available LLMs on the <a
                        href="https://ollama.com/library">Ollama website</a> and download any model you want (or
                    your computer can
                    deal with).<br>
                    For reference, if you don't know what LLM model to choose (we've all been there) here is a list
                    of models you
                    can try out on consumer hardware:
                </p><br>
                <div class="table-wrapper">
                    <table class="alt">
                        <thead>
                        <tr>
                            <th>Computer power</th>
                            <th style="text-align:center">LLM models name to try</th>
                            <th style="text-align:center">LLM quality</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td><strong>Out of this world</strong> <em>(RTX 4090 / 64 GB RAM)</em></td>
                            <td style="text-align:center">llama3.3:70b, gemma3:27b, deepseek-r1:32b, mixtral:8x22b</td>
                            <td style="text-align:center">Excellent reasoning and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>Epic</strong> <em>(RTX 4090 / 32 GB RAM)</em></td>
                            <td style="text-align:center">llama3.1:8b, gemma3:27b, dolphin-mixtral:8x7b, dolphin-mixtral:8x7b-v2.5-q6_K
                            </td>
                            <td style="text-align:center">Good reasoning and instruction following. (q6_K model
                                should be less
                                consuming than the default Mixtral if you have any issues)
                            </td>
                        </tr>
                        <tr>
                            <td><strong>Gamer</strong> <em>(GTX 1080TI / 16 GB RAM)</em></td>
                            <td style="text-align:center">llama3.1:8b, mistral:7b</td>
                            <td style="text-align:center">Llama still works but is slower. Expect limited
                                reasoning
                                and no more than
                                2 complex instructions at a time
                            </td>
                        </tr>
                        <tr>
                            <td><strong>Potato</strong></td>
                            <td style="text-align:center">phi:2.7b, phi3:3.8b, tinyllama:1.1b</td>
                            <td style="text-align:center">Almost no reasoning, incapable of following more than
                                1
                                instruction at a
                                time, English bound only ; Dumb as a stone
                            </td>
                        </tr>
                        </tbody>
                    </table>
                </div>
                <p>
                    If you are still unsure what LLM to pick remember that loading an LLM won't destroy your
                    computer.<br>
                    If the model is too large, the inference server will abort or crash, and then you can just delete
                    the LLM and install a smaller one.<br>
                    To help you choose an LLM that fits you can use this <a
                        href="https://apxml.com/tools/vram-calculator">online calculator</a>.
                </p><br>
                <h4>Understanding model names</h4>
                <div>
                    ▶️ <b>Parameters size</b><br><br>
                    Large Language Models have a default number of parameters. For instance
                    <a href="https://ollama.com/library/llama3.1">Lama3.1</a> has 3 different versions: "8B", "70B"
                    and "405B".<br>
                    <b>=><i>8B</i> means <i>8 Billon parameters</i></b> and the higher, the smarter. But it also means
                    that
                    it will consume more RAM.<br>
                    Most consumer hardware can load "8B" parameters models but not much higher.<br><br>

                    ▶️ <b>Quantization Overview</b><br><br>

                    Next, let’s talk about quantization. Every model has a
                    <a href="https://ollama.com/library/llama3.1/tags">quantized version of itself</a>.<br>
                    For example, the model <i>llama3.1:8b-instruct-q4_K_S</i> is quantized to 4 bits, that’s what the
                    <i>-q4</i> means.<br><br>

                    The lower the number after the <i>q</i>, the more heavily quantized the model is. Heavier quantization reduces the model’s RAM usage on your machine.<br>
                    By default, models use full precision (16-bit), indicated by <i>-fp16</i> in their name.<br>
                    You can load a lighter version like <i>-q8</i>, which has half the precision of <i>fp16</i>, uses half the memory, but may also perform slightly worse.<br><br>

                    The great thing about quantization is that the drop in performance isn’t directly proportional to the reduction in size.<br>
                    This means it's often better to run a larger model in a quantized form than a smaller model in full precision.<br><br>

                    For instance, <i>llama3.1:70b-instruct-q4_0</i> (LLaMA 3.1 with 70 billion parameters, fine-tuned for instruction-following, quantized to 4 bits) will generally outperform <i>llama3.1:8b-instruct-fp16</i> (LLaMA 3.1 with only 8 billion parameters in full precision), while using similar or even less RAM.<br><br>

                    <b>In short:</b> it's usually better to use a larger, quantized model than a smaller,
                    full-precision one, if it fits in your system’s RAM.<br><br>

                    <b>Note:</b> On Ollama, the <i>latest</i> tag usually points to the <i>8B q4</i> model. You can
                    confirm this by checking the hash under the model name, both <i>latest</i> and <i>8b-q4</i> will share the same hash.

                </div><br>
                <h4>Computer's RAM VS Graphic Cards' RAM</h4>
                <div>
                     ▶️ When using a PC, it has its own native RAM, the classic system memory you're already familiar
                    with.<br>
                    But if you have a dedicated graphics card, it also comes with its own type of memory called <b>VRAM</b> (Video RAM).<br>
                    VRAM is faster than regular RAM because it can be accessed directly, without needing to go through the operating system’s kernel.<br><br>

                    A program that requires large amounts of memory and can use VRAM will typically run faster. This is why inference servers like Ollama try to load the LLM model directly into VRAM.<br><br>

                    ▶️ <b>What happens if the model is too large to fit in VRAM?</b><br>
                    That’s where <b>shared GPU memory</b> comes in!<br>
                    This is a portion of your system’s regular RAM that the graphics card can use as if it were its own. From Ollama’s perspective, it will look like your GPU has more VRAM than it actually does.<br>
                    However, since this memory still goes through the kernel, it's significantly slower than true VRAM.<br>
                    As a result, using shared GPU memory can cause performance drops, and it’s generally best to stay within the limits of your available VRAM whenever possible.<br><br>

                    ▶️ On Macs, the memory is <b>unified</b>, meaning the CPU and GPU share the same pool of RAM.
                    This architecture allows both the graphics processor and the main processor to access memory
                    quickly and seamlessly, so there's no distinction between RAM and VRAM ; it's always fast.

                </div>

                <hr class="major"/>

                <h2 id="running-the-model">Running the model</h2>

                <p>When you have chosen your model it's time to use the Ollama CLI to pull it on your computer.</p>
                <ul>
                    <li>To download the model do <code>ollama pull &lt;model_name&gt;</code> ;</li>
                    <li>Then list installed models using <code>ollama list</code> ;</li>
                    <li>When ready, test the model locally by doing <code>ollama run &lt;model_name&gt;</code> which
                        will start a conversation with the LLM ;
                    </li>
                </ul>

                <hr class="major"/>

                <h2 id="installing-yacana">Installing Yacana</h2>
                <p style="text-align: center;">
						<span class="image"><img style="width: 50%;" src="../images/yacana_round_small.png"
                                                 alt=""/></span>
                </p>
                <pre><code class="language-python">
pip install yacana==0.3.2
					</code></pre>

                <hr class="major"/>

                <h2 id="imports">Imports</h2>
                <p>
                    When using other frameworks 'import hell' quickly appears. To prevent this bothersome problem we
                    propose that you always import all of Yacana's modules and when finished developing let the IDE
                    remove the unused imports.
                    Unused imports generally appear grayed. Thus, we recommend that you prepend these imports in all
                    your files and clean them later. This way the IDE will have auto-completion available and will
                    help you develop 10 times faster.
                </p>
                <pre><code class="language-python">
                    from yacana import OllamaAgent, OpenAiAgent, GenericAgent, Task, Tool, ToolType, Message, GenericMessage, MessageRole, GroupSolve, EndChat, EndChatMode, OllamaModelSettings, OpenAiModelSettings, LoggerManager, ToolError, MaxToolErrorIter
                </code></pre>


                <div style="text-align: center; margin-top: 50px;">
                    <h4>Pagination</h4>
                    <ul class="pagination">
                        <li><span class="button disabled">Prev</span></li>

                        <li><a href="agents_and_tasks.html#creating-an-agent" class="button">Next</a></li>
                    </ul>
                </div>
            </section>

        </div>
    </div>

    <!-- Sidebar -->
    <div id="sidebar">
        <div class="inner">

            <!-- Search -->
            <section id="search" class="alt">
                <form method="post" action="#">
                    <input type="text" name="query" id="query" placeholder="Search"/>
                </form>
            </section>

            <!-- Menu Container -->
            <div id="menu-container"></div>

            <!-- Section -->
            <section>
                <div class="page-nav-container">
                    <!-- Dynamic page navigation will be inserted here -->
                </div>
            </section>

            <!-- Section -->
            <section>
                <header class="major">
                    <h2>Related Youtube video</h2>
                </header>
                <div class="mini-posts">
                    <article>
                        <a href="#" class="image"><img src="../images/youtube_down.jpg" alt=""/></a>
                        <p>Youtube video for this section is still under creation. Please be patient ^^</p>
                    </article>
                </div>
            </section>

            <!-- Footer -->
            <footer id="footer">
                <p class="copyright">&copy; Emilien Lancelot. All rights reserved.<br>
                    Design: <a href="https://html5up.net">HTML5UP</a>.</p>
            </footer>

        </div>
    </div>

</div>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script src="../assets/js/menu.js"></script>
<script>
    // Initialize both menus when the document is ready
    $(document).ready(function() {
        initializeMainNavMenu();
        initializePageNavMenu();
    });
</script>

</body>

</html>
