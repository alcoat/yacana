<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Yacana - Installation</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="../assets/css/main.css"/>
    <link rel="stylesheet" href="../assets/css/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/monokai.min.css">
    <link rel="stylesheet" href="../assets/css/foldgutter.min.css">
    <link rel="stylesheet" href="../assets/css/codemirror-custom.css">
    <link rel="stylesheet" href="../assets/css/zenburn.min.css">
    <script src="../assets/js/codemirror.min.js"></script>
    <script src="../assets/js/python.min.js"></script>
    <script src="../assets/js/json-lint.min.js"></script>
    <script src="../assets/js/foldcode.min.js"></script>
    <script src="../assets/js/foldgutter.min.js"></script>
    <script src="../assets/js/brace-fold.min.js"></script>
    <script src="../assets/js/codemirror-custom.js"></script>
</head>

<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">
        <div class="inner">

            <!-- Header -->
            <header id="header">
                <a href="../index.html" class="logo"><strong>Yacana</strong>, powering open source LLMs</a>
                <ul class="icons">
                    <li><a href="https://x.com/RSoftwares_ofc" class="icon brands fa-twitter"><span class="label">Twitter</span></a>
                    </li>
                    <li><a href="https://medium.com/@docteur_rs" class="icon brands fa-medium-m"><span class="label">Medium</span></a>
                    </li>
                    <li><a href="https://www.youtube.com/channel/UCvi7R0CRmtxhWOVw62XteTw"
                           class="icon brands fa-youtube"><span class="label">Medium</span></a></li>
                    <li><a href="https://github.com/rememberSoftwares/yacana" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                </ul>
            </header>

            <!-- Content -->
            <section>
                <header class="main">
                    <h1 id="installing-ollama">VIII. OpenAI inference servers</h1>
                </header>

                <span class="image main"><img src="../images/other_inf_servers_banner.png"
                                              alt="Yacana and Ollama installation"/></span>


                <h2 id="compatibility-with-openai-api">Compatibility with OpenAI API</h2>
                <p>
                    Yacana was initially designed to work only with Ollama. However, many projects require mixing both private LLM providers and local open source models to achieve
                    a greater production grade product. Private LLM providers like OpenAI or Anthropic are great for production due to their quality but cost a lot of money.
                    On the other hand, local open source models are way cheaper to run but their quality is not always there. Hence having the ability to mix both is a great asset.
                </p>
                <br>
                <p>
                    The force of Yacana is to provide you with the same programming API whether you use Ollama or an OpenAI-compatible endpoint.
                    <br>
                    To be fair there is one important difference between the OllamaAgent and OpenAiAgent : the way tools are called.
                    <br>
                    For Ollama, tools are called using an "enhanced tool calling" system where Yacana will iterate over the tools and call the appropriate one with its own 
                    internal method. This system was made specifically for local LLMs to achieve higher call success rates.
                    <br>
                    For OpenAI, tools are called following the OpenAI standard. So, when using ChatGPT you won't have any troubles calling tools as chatGPT is tailored for this.
                    However, when using other inference servers like VLLM you will have a lower success rate at calling tools. This is a little bit unfortunate and will be addressed in
                    another update. We will make the OllamaAgent able to use the OpenAi tool calling standard and allow the OpenAIAgent to benefit from the enhanced tool calling capabilities.
                    <br>
                    Stay tuned for future updates!
                </p>
                <br>
                <h2 id="using-chatgpt">Using ChatGPT</h2>
                <p>
                    To use ChatGPT, you can do:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent

openai_agent = OpenAiAgent("AI assistant", "gpt-4o-mini", system_prompt="You are a helpful AI assistant", api_token="sk-proj-XXXXXXXXXXXXXXX")

# Use the agent to solve a task
message: GenericMessage = Task("What is the capital of France?", openai_agent).solve()
print(message.content)
</code></pre>

                <h2 id="using-vllm">Using VLLM</h2>
                <h4>Installation</h4>
                <p>
                    To help you go through the installation process on WSL you can follow this tutorial: <a href="https://itnext.io/making-vllm-work-on-wsl2-61182235424f">Installing VLLM on WSL</a>.
                </p>
                <br>
                <p>
                    First, let's install the VLLM.
                    <br>
                    You can find the detailed installation steps on the <a href="https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html">VLLM documentation</a>.
                    <br>
                    We recommand you use conda to install VLLM. Note that conda does not allow you to use it in an enterprise environment without paying for a license. You can use 'uv' instead.
                    <br>
                    Once conda is installed you can simply pip install vllm.
                    <pre><code class="language-python">
conda create -n vllm python=3.12 -y
conda activate vllm
pip install vllm
                    </code></pre>
                </p>
                <p>
                    Now, let's start the inference server with a model. If it's not already present it will be downloaded.
                    <br>
                    We'll use the <a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct">Llama-3.2-1B-Instruct model</a>.
                    <br>
                </p>
                <pre><code class="language-python">
vllm serve meta-llama/Llama-3.2-1B-Instruct --guided-decoding-backend outlines --enable-auto-tool-choice --tool-call-parser llama3_json
                </code></pre>
                <br>
                <p>
                    For the inference to start you will need a HuggingFace Account to validate the Facebook's license agreement.
                    <br>
                    Read the VLLM tutorial if you need help with that step.
                </p>
                <br>
                <p>
                    About the vllm command line parameters:
                    <ul>
                        <li>
                            <b>guided-decoding-backend</b>: This parameter is used to specify the backend for guided decoding. It's used for structured outputs.
                        </li>
                        <li>
                            <b>enable-auto-tool-choice</b>: This parameter is used to enable the auto tool choice. Required when at least one tool is defined as optional.
                        </li>
                        <li>
                            <b>tool-call-parser</b>: This parameter is used to specify what shema to expect when doing tool calls. As we are using facebook's llama we will use the "llama3_json" parser.
                        </li>
                    </ul>
                </p>
                <p>
                    Once the inference server is running, you can use the following code to create an OpenAI-compatible agent:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent

vllm_agent_2 = OpenAiAgent("AI assistant", "meta-llama/Llama-3.2-1B-Instruct", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:8000/v1", api_token="leave blank", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})

# Use the agent to solve a task
message: GenericMessage = Task("What is the capital of France?", vllm_agent).solve()
print(message.content)
                </code></pre>
                <br>
                <h4>Tool calling</h4>
                <p>
                    Doing simple tool calling:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent

vllm_agent = OpenAiAgent("AI assistant", "meta-llama/Llama-3.2-1B-Instruct", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:8000/v1", api_token="leave blank", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})

# Defining a fake weather tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny with a high of 25Â°C."

# Defining the tool
get_weather_tool = Tool("Get_weather", "Calls a weather API and returns the current weather in the given city.", get_weather)

# Solving a task
extra_body = {
    'guided_decoding_backend': 'outlines'
}
Task("What's the weather in paris ?", vllm_agent, tools=[get_weather_tool], runtime_config={"extra_body": extra_body}).solve()

vllm_agent.history.pretty_print()
                </code></pre>
                <p>
                    Note how we used the <code>runtime_config</code> parameter to specify the guided decoding backend. You can use this parameter to specify other parameters as well.
                    This is a direct access to the underlying library. For OpenAI we use the <a href="https://github.com/openai/openai-python">OpenAI python client</a>.
                </p>
                <br>
                <h4>Structured output</h4>
                <p>
                    For structured output you can do:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent, GenericMessage, Task

class CountryFact(BaseModel):
    name: str
    fact: str

class Facts(BaseModel):
    countryFacts: list[CountryFact]


vllm_agent = OpenAiAgent("AI assistant", "meta-llama/Llama-3.2-1B-Instruct", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:8000/v1", api_token="leave blank", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})

message: GenericMessage = Task("Tell me 3 facts about Canada.", vllm_agent, structured_output=Facts).solve()

# Print the content of the message as a JSON string
print(message.content)
# Print the structured output as a real class instance
print("Name = ", message.structured_output.countryFacts[0].name)
print("Fact = ", message.structured_output.countryFacts[0].fact)
                </code></pre>
        </div>
    </div>

    <!-- Sidebar -->
    <div id="sidebar">
        <div class="inner">

            <!-- Search -->
            <section id="search" class="alt">
                <form method="post" action="#">
                    <input type="text" name="query" id="query" placeholder="Search"/>
                </form>
            </section>

            <!-- Menu Container -->
            <div id="menu-container"></div>

            <!-- Section -->
            <section>
                <header class="major">
                    <h2>Related Youtube video</h2>
                </header>
                <div class="mini-posts">
                    <article>
                        <a href="#" class="image"><img src="../images/youtube_down.jpg" alt=""/></a>
                        <p>Youtube video for this section is still under creation. Please be patient ^^</p>
                    </article>
                </div>
            </section>

            <!-- Section -->
            <section>
                <div class="page-nav-container">
                    <!-- Dynamic page navigation will be inserted here -->
                </div>
            </section>

            <!-- Footer -->
            <footer id="footer">
                <p class="copyright">&copy; Emilien Lancelot. All rights reserved.<br>
                    Design: <a href="https://html5up.net">HTML5UP</a>.</p>
            </footer>

        </div>
    </div>

</div>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script src="../assets/js/menu.js"></script>
<script>
    // Initialize both menus when the document is ready
    $(document).ready(function() {
        initializeMainNavMenu();
        initializePageNavMenu();
    });
</script>

</body>

</html>





