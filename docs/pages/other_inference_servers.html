<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Yacana - Installation</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="../assets/css/main.css"/>
    <link rel="stylesheet" href="../assets/css/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/monokai.min.css">
    <link rel="stylesheet" href="../assets/css/foldgutter.min.css">
    <link rel="stylesheet" href="../assets/css/codemirror-custom.css">
    <link rel="stylesheet" href="../assets/css/zenburn.min.css">
    <script src="../assets/js/codemirror.min.js"></script>
    <script src="../assets/js/python.min.js"></script>
    <script src="../assets/js/json-lint.min.js"></script>
    <script src="../assets/js/foldcode.min.js"></script>
    <script src="../assets/js/foldgutter.min.js"></script>
    <script src="../assets/js/brace-fold.min.js"></script>
    <script src="../assets/js/codemirror-custom.js"></script>
</head>

<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">
        <div class="inner">

            <!-- Header -->
            <header id="header">
                <a href="../index.html" class="logo"><strong>Yacana</strong>, powering open source LLMs</a>
                <ul class="icons">
                    <li><a href="https://x.com/RSoftwares_ofc" class="icon brands fa-twitter"><span class="label">Twitter</span></a>
                    </li>
                    <li><a href="https://medium.com/@docteur_rs" class="icon brands fa-medium-m"><span class="label">Medium</span></a>
                    </li>
                    <li><a href="https://www.youtube.com/channel/UCvi7R0CRmtxhWOVw62XteTw"
                           class="icon brands fa-youtube"><span class="label">Medium</span></a></li>
                    <li><a href="https://github.com/rememberSoftwares/yacana" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                </ul>
            </header>

            <!-- Content -->
            <section>
                <header class="main">
                    <h1 id="installing-ollama">VIII. OpenAI inference servers</h1>
                </header>

                <span class="image main"><img src="../images/other_inf_servers_banner.png"
                                              alt="Yacana and Ollama installation"/></span>


                <h2 id="compatibility-with-openai-api">Compatibility with OpenAI API</h2>
                <p>
                    Yacana was initially designed to work only with Ollama. However, many projects require mixing both private LLM providers and local open source models to achieve
                    a greater production grade product. Private LLM providers like OpenAI or Anthropic are great for production due to their quality but cost a lot of money.
                    On the other hand, local open source models are way cheaper to run but their quality is not always there. Hence having the ability to mix both is a great asset.
                </p>
                <br>
                <p>
                    The force of Yacana is to provide you with the same programming API whether you use Ollama or an OpenAI-compatible endpoint.
                    <br>
                    To be fair there is one important difference between the OllamaAgent and OpenAiAgent : the way tools are called.
                    <br>
                    For Ollama, tools are called using an "enhanced tool calling" system where Yacana will iterate over the tools and call the appropriate one with its own 
                    internal method. This system was made specifically for local LLMs to achieve higher call success rates.
                    <br>
                    For OpenAI, tools are called following the OpenAI standard. So, when using ChatGPT you won't have any troubles calling tools as chatGPT is tailored for this.
                    However, when using other inference servers like VLLM you will have a lower success rate at calling tools. This is a little bit unfortunate and will be addressed in
                    another update. We will make the OllamaAgent able to use the OpenAi tool calling standard and allow the OpenAIAgent to benefit from the enhanced tool calling capabilities.
                    <br>
                    Stay tuned for future updates!
                </p>
                <br>
                <h2 id="using-chatgpt">Using ChatGPT</h2>
                <p>
                    To use ChatGPT, you can do:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent, Task, GenericMessage

openai_agent = OpenAiAgent("AI assistant", "gpt-4o-mini", system_prompt="You are a helpful AI assistant", api_token="sk-proj-XXXXXXXXXXXXXXX")

# Use the agent to solve a task
message: GenericMessage = Task("What is the capital of France?", openai_agent).solve()
print(message.content)
                </code></pre>

                <h4>Using tools</h4>

                <pre><code class="language-python">
from yacana import OpenAiAgent, Task, GenericMessage

openai_agent = OpenAiAgent("AI assistant", "gpt-4o-mini", system_prompt="You are a helpful AI assistant", api_token="sk-proj-XXXXXXXXXXXXXXX")

# Use the agent to solve a task
message: GenericMessage = Task("What is the capital of France?", openai_agent).solve()
print(message.content)
                </code></pre>

                We can see by looking at the output that the tool calling is more concise than the one done by the OllamaAgent. This effectivness can only work with 
                very smart LLMs like chatGPT.
                <br>
                When using the OpenAiAgent with an inference server different from OpenAi, your LLM might not be able to call tools correctly. Using the enhaced tool calling for the
                OpenAIAgent will be addressed in a future update.

                <h4>Configuring model settings</h4>

                Yacana offers a class to configure the LLM. The class was made to aggregate all the parameters understood by the OpenAI API.
                <br>
                Note that it was made specifically for chatGPT and some settings may not work with other inference servers.
                <br>
                In the following example we will use the <code>OpenAiModelSettings</code> class to configure the LLM. Using <code>logprobs</code> and <code>top_logprobs</code>
                the OpenAI library will return the 3 best candidates for each token.
                <br>
                However, Yacana was not meant to parse the logprobs in particular. So, we'll use the <code>.raw_llm_json</code> member to access the raw JSON output of the LLM.
                In there we'll find all the information we need.
                <br>
                <code class="language-python"><pre>
from yacana import OpenAiAgent, Task, GenericMessage, OpenAiModelSettings, HistorySlot

model_settings = OpenAiModelSettings(temperature=0, logprobs=True, top_logprobs=3)

openai_agent = OpenAiAgent("AI assistant", "gpt-4o-mini", model_settings=model_settings, system_prompt="You are a helpful AI assistant", api_token="sk-proj-XXXXXXXXXXXXXXXXX")

Task("Tell me 1 facts about Canada.", openai_agent).solve()

slot: HistorySlot = openai_agent.history.get_last_slot()

print("raw_llm_json :", slot.raw_llm_json)
                </pre></code>

                If you are unfamiliar with the HistorySlot object, let me explain. It's a class that wraps the message from the LLM. There are very few usecases where you'll need to
                interact with it. When you need to access a Message from the history in reality you go through the HistorySlot object before acccessing the Message object.
                <br>
                However, in this case the <code>logprobs</code> data is not available inside the <code>Message</code> object itself. So, we'll use the <code>.raw_llm_json</code> 
                member from the surrounding slot to access the raw JSON output of the LLM and get a look at the logprobs.
                <br>

                <h4>Using multiple medias</h4>
                <p>
                    Using medias with ChatGPT is the same than using the OllamaAgent with the difference being that ChatGPT supports multiple medias in a single request.
                    <pre><code class="language-python">
from yacana import Task, OpenAiAgent, GenericMessage

openai_agent = OpenAiAgent("AI assistant", "gpt-4o-mini", system_prompt="You are a helpful AI assistant", api_token="sk-proj-XXXXXXXXXXXXXXXXX")

Task("Describe this image", openai_agent, medias=["./tests/assets/burger.jpg", "./tests/assets/flower.png"]).solve()
                    </code></pre>
                    ⚠️ Images are transformed into base64 before being sent to the LLM. This means that the size of the request will be quite large and you will pay for a lot of tokens.
                </p>

                <h3>Getting alternative responses</h3>
                <p>
                    ChatGPT has a feature allowing it to return multiple version of the same message. It's used to provide you with alternative responses. For instance,
                    some could be more formal, some could be more creative, etc.
                    <br>
                    Yacana offers a way to get these alternative responses.
                    <br>
                    Let's say we want to get 3 alternative responses to the message "What is the capital of France ?"
                </p>
                <pre><code class="language-python">
from typing import List
from yacana import Task, OpenAiAgent, GenericMessage, OpenAiModelSettings, HistorySlot

openai_agent = OpenAiAgent("AI assistant", "gpt-4o-mini", system_prompt="You are a helpful AI assistant", api_token="sk-proj-XXXXXXXXXXXXXXXXX")

# Requesting 3 alternative responses using "n" parameter
model_settings = OpenAiModelSettings(n=3)

Task("What is the capital of France ?", openai_agent, model_settings=model_settings).solve()

# Getting the last slot from the history
slot: HistorySlot = openai_agent.history.get_last_slot()

# Getting the messages from the slot
messages: List[GenericMessage] = slot.messages

# Printing the messages
for message in messages:
    print(message.content)
                </code></pre>
                <p>
                    You might be wandering what is an HistorySlot? It's a class that wraps the message from the LLM.<br>
                    There are very few usecases where you'll need to use it so it has been hided from the user. For instance, when adding a message to the history you're secretly adding an HistorySlot and then<br>
                    a message to that slot. You can see the history as a list of slots instead of a list of messages. In each slot you'll find one or more messages. Most of the time there will only be one 
                    because only OpenAi supports the "n" parameter hence making the functionnality  pretty much useless for other inference server.<br>
                    However, you can create your own slots and add messages to them yourself if the need arises.<br>
                    Finally, how does the slot knows what message to use by default when it has more than one ? It's simple, by default it's the first one.
                    To change the default message you can use the <code>my_slot.set_main_message_index(n: int)</code> method to set the correct index in the list of messages.<br>
                    In the above example we've used the <code>get_last_slot()</code> method to get the last slot from the history and then we've accessed the list of messages inside the slot.<br>
                    We've then used a for loop to print the content of each message. These messages could be given to a different agent to achieve different results. Maybe select the message
                    with the most creative answer. Let's say it's the third one.<br>
                    To make the third message the default one we can do:
                    <pre><code class="language-python">
slot.set_main_message_index(3)
                    </code></pre>
                    Then, when Yacana sends the current history to the LLM to solve another task it will use the third message as the main one. The other messages are still in the slot, 
                    but unused.
                </p>


                <h2 id="using-vllm">Using VLLM</h2>
                <h4>Installation</h4>
                <p>
                    To help you go through the installation process on WSL you can follow this tutorial: <a href="https://itnext.io/making-vllm-work-on-wsl2-61182235424f">Installing VLLM on WSL</a>.
                </p>
                <br>
                <p>
                    First, let's install the VLLM.
                    <br>
                    You can find the detailed installation steps on the <a href="https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html">VLLM documentation</a>.
                    <br>
                    We recommand you use conda to install VLLM. Note that conda does not allow you to use it in an enterprise environment without paying for a license. You can use 'uv' instead.
                    <br>
                    Once conda is installed you can simply pip install vllm.
                    <pre><code class="language-python">
conda create -n vllm python=3.12 -y
conda activate vllm
pip install vllm
                    </code></pre>
                </p>
                <p>
                    Now, let's start the inference server with a model. If it's not already present it will be downloaded.
                    <br>
                    We'll use the <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.2-1B-Instruct model</a>.
                    <br>
                </p>
                <pre><code class="language-python">
vllm serve meta-llama/Llama-3.1-8B-Instruct --max-model-len 8192 --guided-decoding-backend outlines --enable-auto-tool-choice --tool-call-parser llama3_json
                </code></pre>
                <br>
                <p>
                    For the inference to start you will need a HuggingFace Account to validate the Facebook's license agreement.
                    <br>
                    Read the VLLM tutorial if you need help with that step.
                </p>
                <br>
                <p>
                    About the vllm command line parameters:
                    <ul>
                        <li>
                            <b>guided-decoding-backend</b>: This parameter is used to specify the backend for guided decoding. It's used for structured outputs.
                        </li>
                        <li>
                            <b>enable-auto-tool-choice</b>: This parameter is used to enable the auto tool choice. Required when at least one tool is defined as optional.
                        </li>
                        <li>
                            <b>tool-call-parser</b>: This parameter is used to specify what shema to expect when doing tool calls. As we are using facebook's llama we will use the "llama3_json" parser.
                        </li>
                    </ul>
                </p>
                <p>
                    Once the inference server is running, you can use the following code to create an OpenAI-compatible agent:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent

vllm_agent_2 = OpenAiAgent("AI assistant", "meta-llama/Llama-3.1-8B-Instruct", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:8000/v1", api_token="leave blank", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})

# Use the agent to solve a task
message: GenericMessage = Task("What is the capital of France?", vllm_agent).solve()
print(message.content)
                </code></pre>
                <br>
                <h4>Tool calling</h4>
                <p>
                    Doing simple tool calling:
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent

vllm_agent = OpenAiAgent("AI assistant", "meta-llama/Llama-3.1-8B-Instruct", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:8000/v1", api_token="leave blank", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})

# Defining a fake weather tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny with a high of 25°C."

# Defining the tool
get_weather_tool = Tool("Get_weather", "Calls a weather API and returns the current weather in the given city.", get_weather)

# Adding runtime configuration to the underlying OpenAi library so it works with VLLM
extra_body = {
    'guided_decoding_backend': 'outlines',
    'tool_choice': 'auto',
    'enable_auto_tool_choice': True,
    'tool_call_parser': 'auto'
}
Task("What's the weather in paris ?", vllm_agent, tools=[get_weather_tool], runtime_config={"extra_body": extra_body}).solve()

vllm_agent.history.pretty_print()
                </code></pre>
                <p>
                    Note how we used the <code>runtime_config</code> parameter to specify the guided decoding backend. You can use this parameter to specify other parameters as well.
                    This is a direct access to the underlying library.
                    <br>
                    For OpenAI we use the <a href="https://github.com/openai/openai-python">OpenAI python client</a>. You can set any parameter supported by the OpenAI python client.
                    <br>
                    These settings can either be set at the Agent level or at the Task level. For more information 
                    please refer to the <a href="agents_and_tasks.html#accessing-the-underlying-client-library">Accessing the underlying client library</a> section.
                </p>
                <br>
                <h4>Using structured outputs</h4>
                <p>
                    Using structured outputs is the same as the OllamaAgent. This is the power of Yacana. It provides you with the same API for structured outputs on
                    local LLMs as on OpenAI. However, you still need to provide the <code>outline</code> parameter. In this example we set it at the Agent level because it will be
                    usefull for every future task requiring grammar enforcement.
                </p>
                <pre><code class="language-python">
from yacana import OpenAiAgent, GenericMessage, Task

class CountryFact(BaseModel):
    name: str
    fact: str

class Facts(BaseModel):
    countryFacts: list[CountryFact]

vllm_agent = OpenAiAgent("AI assistant", "meta-llama/Llama-3.1-8B-Instruct", system_prompt="You are a helpful AI assistant", endpoint="http://127.0.0.1:8000/v1", api_token="leave blank", runtime_config={"extra_body": {'guided_decoding_backend': 'outlines'}})

message: GenericMessage = Task("Tell me 3 facts about Canada.", vllm_agent, structured_output=Facts).solve()

# Print the content of the message as a JSON string
print(message.content)
# Print the structured output as a real class instance
print("Name = ", message.structured_output.countryFacts[0].name)
print("Fact = ", message.structured_output.countryFacts[0].fact)
                </code></pre>
                <p>
                    All other features, like medias, streaming, etc. are also available with the OpenAiAgent and can be used in the exact same way.
                    Please refer to the main documentation for more information.
                </p>
                <br>
        </div>
    </div>

    <!-- Sidebar -->
    <div id="sidebar">
        <div class="inner">

            <!-- Search -->
            <section id="search" class="alt">
                <form method="post" action="#">
                    <input type="text" name="query" id="query" placeholder="Search"/>
                </form>
            </section>

            <!-- Menu Container -->
            <div id="menu-container"></div>

            <!-- Section -->
            <section>
                <header class="major">
                    <h2>Related Youtube video</h2>
                </header>
                <div class="mini-posts">
                    <article>
                        <a href="#" class="image"><img src="../images/youtube_down.jpg" alt=""/></a>
                        <p>Youtube video for this section is still under creation. Please be patient ^^</p>
                    </article>
                </div>
            </section>

            <!-- Section -->
            <section>
                <div class="page-nav-container">
                    <!-- Dynamic page navigation will be inserted here -->
                </div>
            </section>

            <!-- Footer -->
            <footer id="footer">
                <p class="copyright">&copy; Emilien Lancelot. All rights reserved.<br>
                    Design: <a href="https://html5up.net">HTML5UP</a>.</p>
            </footer>

        </div>
    </div>

</div>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script src="../assets/js/menu.js"></script>
<script>
    // Initialize both menus when the document is ready
    $(document).ready(function() {
        initializeMainNavMenu();
        initializePageNavMenu();
    });
</script>

</body>

</html>





