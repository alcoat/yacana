<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Yacana - Managing agents history</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="../assets/css/main.css"/>
    <link rel="stylesheet" href="../assets/css/codemirror.min.css">
    <link rel="stylesheet" href="../assets/css/monokai.min.css">
    <link rel="stylesheet" href="../assets/css/foldgutter.min.css">
    <link rel="stylesheet" href="../assets/css/codemirror-custom.css">
    <link rel="stylesheet" href="../assets/css/zenburn.min.css">
    <script src="../assets/js/codemirror.min.js"></script>
    <script src="../assets/js/python.min.js"></script>
    <script src="../assets/js/json-lint.min.js"></script>
    <script src="../assets/js/foldcode.min.js"></script>
    <script src="../assets/js/foldgutter.min.js"></script>
    <script src="../assets/js/brace-fold.min.js"></script>
    <script src="../assets/js/codemirror-custom.js"></script>
</head>

<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">
        <div class="inner">

            <!-- Header -->
            <header id="header">
                <a href="../index.html" class="logo"><strong>Yacana</strong>, powering open source LLMs</a>
                <ul class="icons">
                    <li><a href="https://x.com/RSoftwares_ofc" class="icon brands fa-twitter"><span class="label">Twitter</span></a>
                    </li>
                    <li><a href="https://medium.com/@docteur_rs" class="icon brands fa-medium-m"><span class="label">Medium</span></a>
                    </li>
                    <li><a href="https://www.youtube.com/channel/UCvi7R0CRmtxhWOVw62XteTw"
                           class="icon brands fa-youtube"><span class="label">Medium</span></a></li>
                    <li><a href="https://github.com/rememberSoftwares/yacana" class="icon brands fa-github"><span
                            class="label">Github</span></a></li>
                </ul>
            </header>

            <!-- Content -->
            <section>
                <header class="main">
                    <h1 id="printing-history">IV. Managing Agents history</h1>
                </header>

                <span class="image main"><img src="../images/managing_agents_history.jpg" alt="Managing agent history"/></span>

                <h2>Introduction to history management</h2>

                <p>As shown in the previous examples, each agent maintains its own message history, which forms its memory. 
                    When a new request is sent to the LLM (e.g., via Ollama), the entire history is forwarded to the inference server. 
                    The LLM responds to the latest prompt, using the context provided by previous messages and, if available, 
                    the initial system prompt.</p>
                <p>This is what an history looks like:</p>
                <span class="image main"><img
                        src="https://github.com/user-attachments/assets/631b634a-8699-4fff-9ac4-06b403c06ae1"
                        alt="history1A"/></span>
                <p><u>There are 4 types of messages:</u></p>
                <span><i>First, there is the "System" prompt:</i></span>
                <ul>
                    <li>1: The optional "System" prompt that, if present, always goes first.</li>
                </ul>
                <span><i>Then it's only an alternation between these two:</i></span>
                <ul>
                    <li>2: The "User" prompts coming from the Task you set.</li>
                    <li>3: The "Assistant" message which is the answer from the LLM.</li>
                </ul>
                <span><i>You may also have noticed a "Tool" message:</i></span>
                <ul>
                    <li>4: The "Tool" message which is is used by the OpenAi standard to use tools. This is abstract by Yacana so you don't have to deal with this fine mess.</li>
                </ul>
                <hr/>
                <div>
                    <span>Unfortunately, sending the whole history to the LLM for each Task to solve has some disadvantages that
                        can not be overturned:</span>
                    <ul>
                        <li>The longer the history, the longer the LLM takes to analyze it and return an answer.</li>
                        <li>Each LLM comes with a maximum token window size. This is the maximum number of words an LLM
                            can analyze in one run, therefore it's maximum memory.
                        </li>
                        <li>One token roughly represents one word or 3/4 of a word. More information on token count per
                            word <a
                                    href="https://winder.ai/calculating-token-counts-llm-context-windows-practical-guide/">here</a>
                            or <a
                                    href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them">here</a>.
                        </li>
                    </ul>
                </div>
                <p>To counteract those negative effects it is recommended you clean the history when possible. For instance, you
                    could use the <code>forget=True</code> parameter in the <code>Task()</code> class so the prompt and the LLM
                    response do not get saved to the history. You'll see there are many ways to preserve the history from useless noise. <br/>
                </p>
                <p>
                    The Agent class comes with a <code>.history</code> property of type <code>History</code>.
                    It exposes methods to manipulate and view the history.
                    For instance, you can use the <code>.pretty_print()</code> method to print the history on the standard output
                    using the classic color scheme:
                    <pre><code class="language-python">
agent1.history.pretty_print()
                    </code></pre>
                    <br/>
                    Or you can use <code>.get_messages_as_dict()</code> which is great for parsing!
                    <pre><code class="language-python">
messages = agent1.history.get_messages_as_dict()
messages[0].content # First message
messages[1].content # Second message
                    </code></pre>
                </p>

                <p>Let's discover all the different ways to interact with an agent's history!</p>

                <h2>Printing History</h2>
                <p>
                    The history class exposes a <code>.pretty_print()</code> method to print the history on the standard output.
                    It uses the classic color scheme to make it easier to read.
                    <br/>
                    Or you can use <code>.get_messages_as_dict()</code> which is great for parsing!
                </p>
                <p>Look at this simple example:</p>
                <pre><code class="language-python">
from yacana import LoggerManager, OllamaAgent, Task

# Let's deactivate automatic logging so that only OUR prints are shown
LoggerManager.set_log_level(None)

agent1 = OllamaAgent("Cook", "llama3.1:8b", system_prompt="You are a pastry chef")

Task("Generate 5 pastry names followed by the associated estimated calorie.", agent1).solve()
Task("Rank the pastries from the lowest calorie count to the largest.", agent1).solve()

print("############## Agent 1 history pretty print ##################")
agent1.history.pretty_print()
print("############## END ##################")

print("")

print("-------------- Agent 1 history dictionnary --------------")
print(str(agent1.history.get_messages_as_dict()))
print("-------------- END --------------")
					</code></pre>
                <br>
                <pre><code class="text-output">
############## Agent 1 history pretty print ##################

[user]:
Generate 5 pastry names followed by the associated estimated calorie.

[assistant]:
Here are 5 pastry names with their associated estimated calorie counts:

1. **Cinnamon Swirl Brioche** (250-300 calories) - A sweet, buttery brioche filled with a gooey cinnamon swirl.
2. **Lemon Lavender Mille-Feuille** (400-450 calories) - Layers of flaky pastry, lemon curd, and lavender cream create a bright and airy dessert.
3. **Chocolate Soufflé Cake** (500-550 calories) - A rich, decadent chocolate cake that rises like a cloud from the oven, served with a scoop of vanilla ice cream.
4. **Raspberry Almond Croissant** (200-250 calories) - Flaky, buttery croissants filled with sweet and tart raspberry jam and topped with sliced almonds.
5. **Pistachio Rosewater Macarons** (150-200 calories) - Delicate, chewy macarons flavored with pistachio and rosewater, sandwiched together with a light and creamy filling.

Note: The estimated calorie counts are approximate and may vary based on specific ingredients and portion sizes used.

[user]:
Rank the pastries from the lowest calorie count to the largest.

[assistant]:
Based on the estimated calorie counts I provided earlier, here are the pastries ranked from lowest to highest:

1. **Pistachio Rosewater Macarons** (150-200 calories)
2. **Raspberry Almond Croissant** (200-250 calories)
3. **Cinnamon Swirl Brioche** (250-300 calories)
4. **Lemon Lavender Mille-Feuille** (400-450 calories)
5. **Chocolate Soufflé Cake** (500-550 calories)

Let me know if you have any other questions!

############## END ##################

-------------- Agent 1 history dictionnary --------------
[{'role': 'system', 'content': 'You are a pastry chef'}, {'role': 'user', 'content': 'Generate 5 pastry names followed by the associated estimated calorie.'}, {'role': 'assistant', 'content': 'Here are 5 pastry names with their associated estimated calorie counts:\n\n1. **Cinnamon Swirl Brioche** (250-300 calories) - A sweet, buttery brioche filled with a gooey cinnamon swirl.\n2. **Lemon Lavender Mille-Feuille** (400-450 calories) - Layers of flaky pastry, lemon curd, and lavender cream create a bright and airy dessert.\n3. **Chocolate Soufflé Cake** (500-550 calories) - A rich, decadent chocolate cake that rises like a cloud from the oven, served with a scoop of vanilla ice cream.\n4. **Raspberry Almond Croissant** (200-250 calories) - Flaky, buttery croissants filled with sweet and tart raspberry jam and topped with sliced almonds.\n5. **Pistachio Rosewater Macarons** (150-200 calories) - Delicate, chewy macarons flavored with pistachio and rosewater, sandwiched together with a light and creamy filling.\n\nNote: The estimated calorie counts are approximate and may vary based on specific ingredients and portion sizes used.'}, {'role': 'user', 'content': 'Rank the pastries from the lowest calorie count to the largest.'}, {'role': 'assistant', 'content': 'Based on the estimated calorie counts I provided earlier, here are the pastries ranked from lowest to highest:\n\n1. **Pistachio Rosewater Macarons** (150-200 calories)\n2. **Raspberry Almond Croissant** (200-250 calories)\n3. **Cinnamon Swirl Brioche** (250-300 calories)\n4. **Lemon Lavender Mille-Feuille** (400-450 calories)\n5. **Chocolate Soufflé Cake** (500-550 calories)\n\nLet me know if you have any other questions!'}]
-------------- END --------------
					</code></pre>
                <br>
                <p>Output speaks for itself. </p>
                <h2 id="creating-and-loading-checkpoints">Creating and loading checkpoints</h2>
                <p>As mentioned earlier it's better to keep the History clean. Too many prompts and unrelated
                    questions will lead to poorer results so if you have the opportunity to scratch some portion
                    then you should. <br/>
                    Yacana allows you to make history snapshots and rollback to any of them. 
                    This is particularly useful when reaching the end of a flow branch and needing to roll back to start a new one.</p>
                <span class="image main"><img
                        src="https://github.com/user-attachments/assets/824d7fa1-c1b1-4434-85e9-dfa261c2c8e3"
                        alt="Checkpoint1A"/></span>
                <p>It is as simple as this: </p>
                <pre><code class="language-python">
# Creating a checkpoint
checkpoint_id: str = agent1.history.create_check_point()
					</code></pre>
                <br>
                <p>The checkpoint_id is merely a unique string identifier that you can use to load back a save. Like this:
                </p>
                <pre><code class="language-python">
# Go back in time to when the checkpoint was created
agent1.history.load_check_point(checkpoint_id)
					</code></pre>
                <br>
                <p class="icon solid fa-info-circle"> Note that you can make a snapshot <b>before</b>
                    rolling back to a previous save.
                    This way you could go back… to the future. ^^ <br/>
                    Are you okay Marty? </p>
                <p>Let's take a concrete example. You have a pastry website that generates pastry recipes. <br/>
                    The flow will look like this: </p>
                <ol>
                    <li>Propose 5 pastry names ;</li>
                    <li>Create a checkpoint ;</li>
                    <li>The user chooses one of the pastries ;</li>
                    <li>We show the associated calories of the selected pastry ;</li>
                    <li>If the user is okay with it we end the program ;</li>
                    <li>If the user is not okay with the calorie count we go back to the checkpoint and propose to
                        choose from the the list again ;
                    </li>
                    <li>Repeat until satisfied ;</li>
                    <li>We'll show the final agent's History and make sure that it ONLY stored the selected pastry ;
                    </li>
                </ol>
                <p>With a bit of color, it would look like this: </p>
                <p style="text-align: center;"><img
                        src="https://github.com/user-attachments/assets/3a4952aa-18f3-4b6d-93c1-85b909cf24f4"
                        alt="pastry1B"/></p>
                <pre><code class="language-python">
from yacana import LoggerManager, OllamaAgent, Task

# Let's deactivate automatic logging so that only OUR prints are shown; Maybe reactivate (to "info") if you want to see what's happening behind the scenes.
LoggerManager.set_log_level(None)

agent1 = OllamaAgent("Cook", "llama3.1:8b", system_prompt="You are a pastry chef")

# Getting a list of pastries
pastries: str = Task("Generate 5 pastry names displayed as a list. ONLY output the names and nothing else.", agent1).solve().content
print(f"Welcome, you may order one of the following pastries\n{pastries}")

#Looping till the user is satisfied
while True:
    print("")

    # Creating our checkpoint to go back in time
    checkpoint_id: str = agent1.history.create_check_point()

    # Asking for one of the pastries from the list
    user_choice: str = input("Please choose one of the above pastries: ")

    # Printing associated calories for the selected pastry
    pastry_calorie_question: str = Task(f"The user said '{user_choice}'. Your task is to output a specific sentence and replace the &#60;replace&#62; tags with the correct values: 'You selected the &#60;replace&#62;selected pastry&#60;/replace&#62;. The average calorie intake for this pastry is &#60;replace&#62;average associated calories for the selected pastry&#60;/replace&#62;. Do you wish to continue ?", agent1).solve().content
    print(pastry_calorie_question)

    # Asking if the user wants to continue
    is_satisfied: str = input("Continue ? ")

    # Basic yes / no router
    router_answer: str = Task(f"The user said '{is_satisfied}'. Evaluate if the user was okay with its order. If he was, ONLY output 'yes', if not only output 'no'.", agent1).solve().content

    if "yes" in router_answer.lower():
        print("Thank you for your order.")
        # The user was satisfied with his choice. Exiting the loop...
        break
    else:
        # The user wants to choose another pastry. Let's go back in time by loading are previous checkpoint!
        agent1.history.load_check_point(checkpoint_id)
        #  Let's go back to the top of the loop
        continue

print("############## Agent 1 history pretty print ##################")
agent1.history.pretty_print()
print("############## END ##################")
					</code></pre>
                <br>
                <p>▶️ Output:</p>
                <pre><code class="text-output">
Welcome, you may order one of the following pastries
1. Whipped Wonders
2. Creamy Confections
3. Flaky Fancies
4. Golden Galettes
5. Sugar Serenades

Please choose one of the above pastries: The Creamy one looks good
You selected the Creamy Confections. The average calorie intake for this pastry is 350-400 calories per serving. Do you wish to continue?
Continue ? no

Please choose one of the above pastries: Hummm. The golden one?
You selected the Golden Galettes. The average calorie intake for this pastry is approximately 250-300 calories per serving. Do you wish to continue?
Continue ? yes
Thank you for your order.

############## Agent 1 history pretty print ##################

[user]:
Generate 5 pastry names displayed as a list. ONLY output the names and nothing else.

[assistant]:
1. Whipped Wonders
2. Creamy Confections
3. Flaky Fancies
4. Golden Galettes
5. Sugar Serenades

[user]:
The user said 'Hummm. The golden one ?'. Your task is to output a specific sentence and replace the <replace> tags with the correct values: 'You selected the <replace>selected pastry</replace>. The average calorie intake for this pastry is <replace>average associated calories for the selected pastry</replace>. Do you wish to continue ?

[assistant]:
You selected the Golden Galettes. The average calorie intake for this pastry is approximately 250-300 calories per serving. Do you wish to continue?

[user]:
The user said 'yes'. Evaluate if the user was okay with the order. If he was, ONLY output 'yes', if not only output 'no'.

[assistant]:
yes

############## END ##################
					</code></pre>
                <br>
                <p>As you can see in the above output, we went for <i>"the creamy one"</i> but when shown the
                    calories,
                    refused to continue… After that, we chose the <i>"Golden Galettes"</i> which was satisfying.
                    Then the program ended with an output of the agent's history. <br/>
                    We can see in the agent's output that it only remembered us choosing the <i>"Golden
                        Galettes"</i> but not
                    the <i>"Creamy Confections"</i>. This is because we loaded the last checkpoint which rolled us
                    back to
                    making our choice again.</p>

                <hr class="major"/>

                <h2 id="Zero-prompt-shot-vs-multi-prompt-shot">Zero-prompt shot vs multi-prompt shot</h2>
                <p>When an LLM struggles to solve a complex task it may be time to
                    give it a little help. </p>
                <p>In large language models, the approach to prompting can significantly influence the model's
                    performance. </p>
                <ul>
                    <li><em>Zero-shot prompting</em> asks the model to complete a task without any prior examples,
                        relying solely on its pre-existing knowledge. This can lead to varied results, especially in
                        more complex tasks.
                    </li>
                    <li><em>One-shot prompting</em> improves accuracy by providing the model with a single example,
                        offering some guidance on how to approach the task.
                    </li>
                    <li><em>Few-shot prompting</em> further enhances performance by supplying multiple examples,
                        allowing the model to have a better understanding of the task's nuances and producing more
                        reliable and accurate results.
                    </li>
                </ul>
                <p>Yacana provides you with a way to add new Messages to the History manually. The History class
                    exposes an <code>.add_message(...)</code> method. <br/>
                    It takes an argument of type <code>Message()</code> with two parameters: a
                    [MessageRole]() enum and the string message itself. </p>
                <p>For example: </p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Message, MessageRole

# Creating a basic agent with an empty history
agent1 = OllamaAgent("AI assistant", "llama3.1:8b")

# We create a fake prompt identified as coming from the user (Thx to `MessageRole.USER`)
user_message = Message(MessageRole.USER, "What's 2+2 ?")

# We create a fake answer identified as coming from the LLM (Thx to `MessageRole.ASSISTANT`)
fake_ai_response = Message(MessageRole.ASSISTANT, "The answer is 4")

# Let's add these two Messages to the Agent's History
agent1.history.add_message(user_message)
agent1.history.add_message(fake_ai_response)

# Print the content of the history
agent1.history.pretty_print()
					</code></pre>
                <br>
                <p>Outputs:</p>
                <pre><code class="text-output">
[user]:
What's 2+2 ?

[assistant]:
The answer is 4
					</code></pre>
                <br>
                <p>The Agent's History successfully contains the two messages we manually added. </p>
                <p class="icon solid fa-info-circle"> The <code>.add_message()</code> always adds new messages
                    at the end of the stack, however you can add messages wherever you want using some other methods. @todo doc
                </p>
                <p>⚠️ Try to keep the alternation of USER and ASSISTANT as this is how "instruct" LLMs have been
                    trained. </p>
                <hr/>
                <p>Let's see a 0-shot example asking for a JSON output extracted from a given sentence: </p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b")

Task(f"Print the following sentence as JSON, extracting the names and rephrasing the actions: 'Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.'", agent1).solve()
					</code></pre>
                <br>
                <p>Outputs:</p>
                <pre><code class="text-output">
INFO: [PROMPT][To: Ai assistant]: Print the following sentence as JSON extracting the names and rephrasing the actions: 'Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.'

INFO: [AI_RESPONSE][From: Ai assistant]: Here is the sentence rewritten in JSON format:
{
	"people": [
		{
			"name": "Marie",
			"action": "walking"
		},
		{
			"name": "Ryan",
			"action": "watching through the window"
		}
	],
	"weather": {
		"condition": "heavy raindrops",
		"sky": "dark sky"
	}
}
Let me know if you'd like me to help with anything else!
					</code></pre>
                <br>
                <p>Not bad but there's noise. We would like to output the JSON and nothing else. No bedside manners.
                    The <code>Let me know if you'd like me to help with anything else!</code> must go. <br/>
                    Let's introduce another optional Task() parameter: <code>json_output=True</code>. This relies on
                    the inference server (Ollama, OpenAI, etc.) to output as JSON. <br/>
                    <p class="icon solid fa-info-circle"> It is preferable to prompt the LLM to "output as JSON" in addition to this option. </p>
                    <p>⚠️ This is only <b>best effort</b> ! Meaning that it may fail at ouputing correct JSON. If you need reliable JSON than go try structured output ! @todo doc</p>
                </p>
                <p>Replace the Task with this one:</p>
                <pre><code class="language-python">
Task(f"Print the following sentence as JSON extracting the names and rephrasing the actions: 'Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.'", agent1, json_output=True).solve()
					</code></pre>
                <br>
                <p>Outputs:</p>
                <pre><code class="text-output">
INFO: [PROMPT][To: Ai assistant]: Print the following sentence as JSON extracting the names and rephrasing the actions: 'Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.'

INFO: [AI_RESPONSE][From: Ai assistant]: {"names": ["Marie", "Ryan"], "actions": {"Marie": "is walking", "Ryan": "is watching"}, "description": [{"location": "window", "activity": "watching"}, {"location": "outdoors", "activity": "pouring raindrops"}]}
					</code></pre>
                <br>
                <p>Way better. No more noise. <br/>
                    However, we would prefer having an array of <code>name</code> and <code>action</code> even for
                    the weather (the name would be <em>sky</em> and the action <em>raining</em>).</p>
                <p>To achieve this let's give the LLM an example of what we expect by making it believe it already
                    outputted it correctly once: </p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task, MessageRole, Message

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b")

# Making a fake valid interaction
agent1.history.add_message(Message(MessageRole.USER, "Print the following sentence as json extracting the names and rephrasing the actions: 'John is reading a book on the porch while the cold wind blows through the trees.'"))
agent1.history.add_message(Message(MessageRole.ASSISTANT, '[{"name": "John", "action": "Reading a book.", "Cold wind": "Blowing through the trees."]'))

Task(f"Print the following sentence as json extracting the names and rephrasing the actions: 'Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.'", agent1).solve()
					</code></pre>
                <br>
                <p>Outputs:</p>
                <pre><code class="text-output">
INFO: [PROMPT][To: Ai assistant]: Print the following sentence as JSON extracting the names and rephrasing the actions: 'Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.'

INFO: [AI_RESPONSE][From: Ai assistant]: [{"name": "Marie", "action": "Walking her dog."}, {"name": "Ryan", "action": "Watching Marie and her dog through the window."}, {"name": "The dark sky", "action": "Pouring down heavy raindrops."}]
					</code></pre>
                <br>
                <p>This is perfect! <br/>
                    (❕ Model temperature may impact performance here. Consider using a lower value.) <br/>
                    You can add multiple fake interactions like this one to cover more advanced use cases and train
                    the LLM on how to react when they happen. It would become multi-shot prompting. </p>
                <hr/>
                <p>You can also do multi-shot prompting with self-reflection. This takes more CPU time because you
                    decompose the task into multiple subtasks but can be beneficial in some scenarios. </p>
                <p>For example: </p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b")

Task('I will give you a sentence where you must extract as JSON all the names and rephrase all the actions. For example in the following sentence: "John is reading a book on the porch while the cold wind blows through the trees." would result in this JSON output: [{"name": "John", "action": "Reading a book."}, {"name": "Cold wind", "action": "Blowing through the trees."}] ', agent1).solve()

Task(f"Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.", agent1, json_output=True).solve()
					</code></pre>
                <br>
                <p>Outputs</p>
                <pre><code class="text-output">
INFO: [PROMPT]: I will give you a sentence where you must extract as JSON all the names and rephrase all the actions. For example in the following sentence: "John is reading a book on the porch while the cold wind blows through the trees." would result in this JSON output: [{"name": "John", "action": "Reading a book."}, {"name": "Cold wind", "action": "Blowing through the trees."}]

INFO: [AI_RESPONSE]: I'm ready to extract the names and rephrase the actions. What's the sentence?

INFO: [PROMPT]: Marie is walking her dog. Ryan is watching them through the window. The dark sky is pouring down heavy raindrops.

INFO: [AI_RESPONSE]: {"name": "Marie", "action": "Walking with her dog."}
					</code></pre>
                <br>
                <p>:-( <br/>
                    In this case, it didn't work very well as only one name was extracted as JSON. But in more
                    complex scenarios we can assure you that letting the LLM reflect on the guideline beforehand,
                    can be very beneficial to solving the task. </p>

                <hr class="major"/>

                <h2 id="saving-agent-state">Saving Agent state</h2>
                <p>Maybe your program needs to start, stop, and then resume where it stopped. For this use case, Yacana
                    provides a way to store an Agent state into a file and load it later. All of the Agent's
                    properties are saved including the History.<br>
                    A real world example could be a chatbot for customers: When the customer leaves the session,
                    the agent's state is saved and can be loaded back when the customer comes back to continue the conversation.
                </p>
                <p>To save an Agent do the following: </p>
                <pre><code class="language-python">
from yacana import OllamaAgent, Task

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b")

Task("What's 2+2 ?", agent1).solve()

# Exporting the agent1 current state to a file called agent1_save.json
agent1.export_to_file("./agent1_save.json")
				</code></pre>
                <br>
                <p>If you look at the file <code>agent1_save.json</code> you'll see something like this:</p>
                <pre><code class="text-output">
{
    "name": "Ai assistant",
    "model_name": "llama3.1:8b",
    "system_prompt": null,
    "model_settings": {},
    "endpoint": "http://127.0.0.1:11434",
    "history": [
        {
            "role": "user",
            "content": "What's 2+2 ?"
        },
        {
            "role": "assistant",
            "content": "The answer to 2+2 is... (drumroll please)... 4!"
        }
    ]
}
					</code></pre>
                <br>
                <p>Now let's load back this agent from the dead using <code>.import_from_file()</code>! <br/>
                    <strong>In another Python file</strong> add this code snippet:
                </p>
                <pre><code class="language-python">
from yacana import GenericAgent, Task

# You can use any of the Agent classes? The GenericAgent is merely the abstract parent class.
agent2: GenericAgent = GenericAgent.import_from_file("./agent1_save.json")

Task("Multiply by 2 the previous result", agent2).solve()
					</code></pre>
                <br>
                <p class="icon solid fa-info-circle"> The <code>.import_from_file(...)</code> works like a
                    factory pattern, returning a new Agent instance.</p>
                <p>▶️ Output:</p>
                <pre><code class="text-output">
INFO: [PROMPT]: Multiply by 2 the previous result

INFO: [AI_RESPONSE]: If we multiply 4 by 2, we get...

8!
					</code></pre>
                <br>
                <p>
                    As you can see when asked to multiply by 2 the previous result, it remembered agent1's result
                    which was 4. Then it performed the multiplication and got us 8. &#127881;
                </p>

                <h2>Moving through history using tags</h2>

                <p>
                    The <code>Message()</code> class exposes a method to attach a tag to it. Using tags, we can get specific messages from the history.
                    When you retrieve a Message instance you can update it's content hence rewritting the history during runtime.
                </p>
                <p>
                    To add tags to your prompt messages use the <code>Task(..., tags=["tag1", "tag2"])</code> class:
                    <pre><code class="language-python">
from yacana import OllamaAgent, Task

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b")

first_message: Message = Task("What's 2+2 ?", agent1, tags=["first_task_prompt"]).solve()
                    </code></pre>
                    <p>
                        This told Yacana to add the tag <code>"first_task_prompt"</code> to the prompt message generated by the task.
                    </p>
                    <p>
                        But you can also tag the LLM's response:
                    </p>
                    <pre><code class="language-python">
...

first_message.add_tag("first_task_response")
                    </code></pre>
                    <p>
                        This told Yacana to add the tag <code>"first_task_response"</code> to the LLM's response.
                    </p>
                    <p>
                        Note that you can also do this on one line:
                    </p>
                    <pre><code class="language-python">
from yacana import OllamaAgent, Task

agent1 = OllamaAgent("Ai assistant", "llama3.1:8b")

# If you don't need the message instance you can do this:
Task("What's 2+2 ?", agent1, tags=["first_task_prompt", "first_task_response"]).solve().add_tag("first_task_response")
                    </code></pre>
                    <p>
                        Let's make a larger example to show how this works. Bellow is a script that will ask 3 numeric related questions to the LLM.<br>
                        It will then change the second question and answer to something completly different.
                        Finally it will ask the LLM what question was not about numbers. And as we updated the second task's response, it should tell us that its the second one.
                    </p>
                    <pre><code class="language-python">
from yacana import OllamaAgent, Task, GenericMessage

agent1 = OllamaAgent("Ai assistant", "llama3.1:latest")

Task("What's 2+2 ?", agent1, tags=["first_task_prompt"]).solve().add_tag("first_task_response")
Task("What's 20+20 ?", agent1, tags=["second_task_prompt"]).solve().add_tag("second_task_response")
Task("What's 200+200 ?", agent1, tags=["third_task_prompt"]).solve().add_tag("third_task_response")

# Let's get the first task's prompt
first_task_prompt: GenericMessage = agent1.history.get_messages_by_tags(["first_task_prompt"])[0]
first_task_response: GenericMessage = agent1.history.get_messages_by_tags(["first_task_response"])[0]
print("First task prompt: ", first_task_prompt.content)
print("First task response: ", first_task_response.content)

# Let's get the second task's response
second_task_prompt: GenericMessage = agent1.history.get_messages_by_tags(["second_task_prompt"])[0]
second_task_response: GenericMessage = agent1.history.get_messages_by_tags(["second_task_response"])[0]
print("Second task prompt: ", second_task_prompt.content)
print("Second task response: ", second_task_response.content)

# Let's change the second task's prompt
second_task_prompt.content = "Why is the sky blue ?"

# Let's change the second task's response
second_task_response.content = "Because of Rayleigh scattering."

# Now let's ask the LLM what question is not about numbers
Task("What question is not about numbers ?", agent1).solve()

# You'll see the answer during runtime with the logging system but let's print the whole history anyway
print("Showing whole history:")
agent1.history.pretty_print()
                    </code></pre>
                    Output:
                    <pre><code class="text-output">
INFO: [PROMPT][To: Ai assistant]: What's 2+2 ?

INFO: [AI_RESPONSE][From: Ai assistant]: The answer is: 4!

INFO: [PROMPT][To: Ai assistant]: What's 20+20 ?

INFO: [AI_RESPONSE][From: Ai assistant]: That's an easy one!

The answer is: 40!

INFO: [PROMPT][To: Ai assistant]: What's 200+200 ?

INFO: [AI_RESPONSE][From: Ai assistant]: Similar to the previous question, but a bit bigger!

The answer is: 400.
First task prompt:  What's 2+2 ?
First task response:  The answer is: 4!
Second task prompt:  What's 20+20 ?
Second task response:  That's an easy one!

The answer is: 40!

INFO: [PROMPT][To: Ai assistant]: What question is not about numbers ?

INFO: [AI_RESPONSE][From: Ai assistant]: All the questions so far have been about numbers!

How about "Why is the sky blue?" - that's not about numbers!
Showing whole history:
[user]:
What's 2+2 ?

[assistant]:
The answer is: 4!

[user]:
Why is the sky blue ?

[assistant]:
Because of Rayleigh scattering.

[user]:
What's 200+200 ?

[assistant]:
Similar to the previous question, but a bit bigger!

The answer is: 400.

[user]:
What question is not about numbers ?

[assistant]:
All the questions so far have been about numbers!

How about "Why is the sky blue?" - that's not about numbers!
                </code></pre>
                <p>
                    Let's make another example. This time we'll count how messages are present between the first LLM's response and the third task prompt.<br>
                    We'll just be counting but you can think of anything, even deleting the messages!
                </p>
                <pre><code class="language-python">
from typing import List

from yacana import OllamaAgent, Task, GenericMessage

agent1 = OllamaAgent("Ai assistant", "llama3.1:latest")

Task("What's 2+2 ?", agent1, tags=["first_task_prompt"]).solve().add_tag("first_task_response")
Task("What's 20+20 ?", agent1, tags=["second_task_prompt"]).solve().add_tag("second_task_response")
Task("What's 200+200 ?", agent1, tags=["third_task_prompt"]).solve().add_tag("third_task_response")

# Getting all messages from the history
messages: List[GenericMessage] = agent1.history.get_all_messages()

# Selecting these two tags to count the number of messages between them
start_tag = "first_task_response"
end_tag = "third_task_prompt"

nb_messages = 0
start_counting = False
for message in messages:
    if "first_task_response" in message.tags:
        start_counting = True
    if "third_task_prompt" in message.tags:
        break
    if start_counting:
        nb_messages += 1

print("Number of messages between the two selected messages: ", nb_messages)                        
                </code></pre>
                <p>
                    Output:
                </p>
                <pre><code class="text-output">
INFO: [PROMPT][To: Ai assistant]: What's 2+2 ?

INFO: [AI_RESPONSE][From: Ai assistant]: The answer is: 4

INFO: [PROMPT][To: Ai assistant]: What's 20+20 ?

INFO: [AI_RESPONSE][From: Ai assistant]: Easy one!

The answer is: 40

INFO: [PROMPT][To: Ai assistant]: What's 200+200 ?

INFO: [AI_RESPONSE][From: Ai assistant]: Similar to the previous ones...

The answer is: 400
Number of messages between the two selected messages:  3
                </code></pre>
                <hr>
                <p>
                    There are a few builtin tags that are automatically added to some messages:
                </p>
                <br>
                <div class="table-wrapper">
                    <table class="alt">
                        <thead>
                        <tr>
                            <th>Tag name</th>
                            <th style="text-align:center">Tag description</th>
                        </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>yacana_builtin</td>
                                <td style="text-align:center">
                                    Added to all messages created by Yacana itself. Usefull to identify messages that were created by a Task
                                    and the ones that were manualy inserted into the history. For example: multi-shot prompts.
                                </td>
                            </tr>
                            <tr>
                                <td>yacana_response</td>
                                <td style="text-align:center">
                                    All responses from the LLM are tagged with this. When using enhanced tool calling there will be multiple messages tagged with this.
                                </td>
                            </tr>
                            <tr>
                                <td>yacana_prompt</td>
                                <td style="text-align:center">All prompts created with a Task are tagged with this.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div style="text-align: center; margin-top: 50px;">
                    <h4>Pagination</h4>
                    <ul class="pagination">
                        <li><a href="routing.html#concepts-of-routing" class="button">Prev</a></li>

                        <li><a href="tool_calling.html#concept-of-calling-tools" class="button">Next</a></li>
                    </ul>
                </div>
            </section>
        </div>
    </div>

    <!-- Sidebar -->
    <div id="sidebar">
        <div class="inner">

            <!-- Search -->
            <section id="search" class="alt">
                <form method="post" action="#">
                    <input type="text" name="query" id="query" placeholder="Search"/>
                </form>
            </section>

            <!-- Menu Container -->
            <div id="menu-container"></div>

            <!-- Section -->
            <section>
                <header class="major">
                    <h2>Related Youtube video</h2>
                </header>
                <div class="mini-posts">
                    <article>
                        <a href="#" class="image"><img src="../images/youtube_down.jpg" alt=""/></a>
                        <p>Youtube video for this section is still under creation. Please be patient ^^</p>
                    </article>
                </div>
            </section>

            <!-- Section -->
            <section>
                <div class="page-nav-container">
                    <!-- Dynamic page navigation will be inserted here -->
                </div>
            </section>

            <!-- Footer -->
            <footer id="footer">
                <p class="copyright">&copy; Emilien Lancelot. All rights reserved.<br>
                    Design: <a href="https://html5up.net">HTML5UP</a>.</p>
            </footer>

        </div>
    </div>

</div>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script src="../assets/js/menu.js"></script>
<script>
    // Load the menu template
    fetch('templates/tuto_nav.html')
        .then(response => response.text())
        .then(html => {
            document.getElementById('menu-container').innerHTML = html;
        });

    // Initialize both menus when the document is ready
    $(document).ready(function() {
        initializeMainNavMenu();
        initializePageNavMenu();
    });
</script>

</body>

</html>
